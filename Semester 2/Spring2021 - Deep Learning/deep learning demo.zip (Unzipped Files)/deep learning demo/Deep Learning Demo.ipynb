{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Deep Learning Demo.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Gy-orqtnjAY0"},"source":["# Pytorch Demo for Deep Learning 2021\n","## Description:\n","Demo contains code to classify cats and dogs dataset from kaggle. Classifier is built using fully connected layers from pytorch nn module. Code is run on google colab and it works correctly. Students are also suggested to take advantge of colab facility. Functionality of different parts is explained a little below. I have shared the dataset with all students you can access it using this link. but you can only access it using your university email.\n","\n","train dataset=https://drive.google.com/drive/folders/16YIhnoi1sgYulknym39KtmetHhCxN2N8?usp=sharing\n","\n","test dataset=https://drive.google.com/drive/folders/1hbC5sWl6Z1WI20u2iX1Yxwzrmzz8RvSp?usp=sharing\n","\n","First cell below imports required packages."]},{"cell_type":"code","metadata":{"id":"ILEhkZDdANAn","executionInfo":{"status":"ok","timestamp":1617561216305,"user_tz":-300,"elapsed":4607,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}}},"source":["import matplotlib.pyplot as plt\n","from torch.autograd import Variable\n","import torch\n","from torch import nn\n","from torch import optim\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms, models\n"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mqgbSMg9jAY_"},"source":["#### Two cells given below import google drive as a folder to colab environment. If you are using your own computer for this demo ignore these cell. comment them out."]},{"cell_type":"code","metadata":{"id":"VJncZ78jKGhQ","executionInfo":{"status":"ok","timestamp":1617561216307,"user_tz":-300,"elapsed":4588,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}}},"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"I0dGcWgtMDz-","executionInfo":{"status":"ok","timestamp":1617561216309,"user_tz":-300,"elapsed":4579,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}}},"source":["# drive.mount(\"/content/drive\", force_remount=True)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ewNB3RPYjAZC"},"source":["## Tranforms and Dataloader\n","#### Code in the cell below loads dataset so that we can train model on it and test our trained model. First two statements define tranforms to transform our dataset. Images are resized to (100*100) and normalized and converted to pytorch tensors. It is not necessary to resize images but it is done for this tutorial to save a lot trianing time. Normalizing datasets have advantages that you guys will learn in future lectures.\n","#### Image folder is loaded using ImageFolder Object and then it is passed as argument to Dataloader Object to load data at run time."]},{"cell_type":"code","metadata":{"id":"Ax3pKtzE5QiU","executionInfo":{"status":"ok","timestamp":1617561216310,"user_tz":-300,"elapsed":4577,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}}},"source":["# !unzip -uq \"/content/drive/MyDrive/datasets/testCD-20210402T171330Z-001.zip\" -d \"/content/drive/MyDrive/datasets/\"\n","\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"C73_5-jy7Qd7","executionInfo":{"status":"ok","timestamp":1617561216311,"user_tz":-300,"elapsed":4575,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}}},"source":["# !unzip -uq \"/content/drive/MyDrive/datasets/train-20210402T171330Z-001.zip\" -d \"/content/drive/MyDrive/datasets/\""],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZlgjkQjjAdNj","executionInfo":{"status":"ok","timestamp":1617561288350,"user_tz":-300,"elapsed":76611,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}}},"source":["data_dir = '/content/drive/MyDrive/datasets' #DOWNLOAD DATA FOR THIS\n","\n","# TODO: Define transforms for the training data and testing data\n","train_transforms = transforms.Compose([transforms.Resize((100,100)),transforms.ToTensor(),\n","                                       transforms.Normalize([0.485, 0.456, 0.406],\n","                                                            [0.229, 0.224, 0.225])])\n","\n","test_transforms = transforms.Compose([transforms.Resize((100,100)),transforms.ToTensor(),\n","                                      transforms.Normalize([0.485, 0.456, 0.406],\n","                                                           [0.229, 0.224, 0.225])])\n","\n","# Loading dataset using ImageFolder method. Pass transforms in here.\n","train_data = datasets.ImageFolder(data_dir + '/train', transform=train_transforms)\n","test_data = datasets.ImageFolder(data_dir + '/testCD', transform=test_transforms)\n","\n","# Here train and test dataloader are created using train_data and test_data objects. Batch size tells how many images are \n","# loaded for one iteration. \n","\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=64)\n"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4BXVbfs3jAZE"},"source":["## Declaring Neural Network\n","#### This cell defines our neural network. Our neural network has five layers. First layer is input layer, then next three layers are hidden layers and last layer is output layer. Input to our network is an (100*100*3) sized image. So our input layer has 30000 neurons, next three layers have 5000, 500 and 100 neurons respectively. Then the last layer has only two neurons because our dataset has only two classes. Constructor of class is just initializing these layers.\n","#### Forward function is compulsery to implement if we are extending nn.Module class. It specify's what happens to the input image. Relu is used as non-linearity in this case. \n"]},{"cell_type":"code","metadata":{"id":"y3tQtohZ0mtS","executionInfo":{"status":"ok","timestamp":1617561288354,"user_tz":-300,"elapsed":76612,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}}},"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.fc1 = nn.Linear(100 * 100*3, 5000)\n","        self.fc2 = nn.Linear(5000, 500)\n","        self.fc3 = nn.Linear(500, 100)\n","        self.fc4 = nn.Linear(100, 2)\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = F.relu(self.fc3(x))\n","        x = self.fc4(x)\n","        return F.log_softmax(x,dim=1)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T5jCFTuKjAZF"},"source":["In this cell we create object of Net class and specify learning rate and number of epochs. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UDmomokK1FwQ","executionInfo":{"status":"ok","timestamp":1617561300379,"user_tz":-300,"elapsed":88626,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}},"outputId":"7e46606d-7aae-49d4-a924-679db84c45cc"},"source":["model = Net()\n","learning_rate=0.0005\n","epochs=10\n","\n","# this statement tell our code if there gpu available on our machine or not.\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Net(\n","  (fc1): Linear(in_features=30000, out_features=5000, bias=True)\n","  (fc2): Linear(in_features=5000, out_features=500, bias=True)\n","  (fc3): Linear(in_features=500, out_features=100, bias=True)\n","  (fc4): Linear(in_features=100, out_features=2, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"zLXtb8S_jAZG"},"source":["In this cell we choose optimizer and loss function. We are using stochastic gradient descent as our optimizer and cross entropy loss as our loss function. You will learning how these algorithms work in class lectures in detail."]},{"cell_type":"code","metadata":{"id":"nOEs_OZS8pz0","executionInfo":{"status":"ok","timestamp":1617561300380,"user_tz":-300,"elapsed":88623,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}}},"source":["optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n","criterion = nn.CrossEntropyLoss()"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CQ5hQVnLjAZH"},"source":["## Training the Network\n","Training neural network is done in multiple steps. After extracting data and target from the train loader, we assign them to the device. Our network takes 1 dimensional input tensor but images are 3 dimensional tensors, so they are converted to 1d tensors using (view) function. torch accumulates gradients of all steps so we have to zero all previous gradients, this is done with zero_grad() function. After this data is passed through network and loss is calculated based on predictions and targets then this loss is passed backward. This is done using loss.backward() function.   "]},{"cell_type":"code","metadata":{"id":"VkE0qG2s83xV","executionInfo":{"status":"ok","timestamp":1617561300381,"user_tz":-300,"elapsed":88621,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}}},"source":["# for epoch in range(epochs):\n","#     for batch_idx, (data, target) in enumerate(train_loader):\n","#         data, target = Variable(data), Variable(target)\n","#         data, target = data.to(device), target.to(device)        \n","#         data = data.view(-1, 100*100*3)\n","#         optimizer.zero_grad()\n","#         net_out = model(data)\n","#         # print('Targets',target)\n","#         # print(\"Prediction\",net_out)\n","#         loss = criterion(net_out, target)\n","#         loss.backward()\n","#         optimizer.step()\n","#         if batch_idx % 10 == 0:\n","#             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","#                     epoch, batch_idx * len(data), len(train_loader.dataset),\n","#                            100. * batch_idx / len(train_loader), loss.item()))"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"tof1HRsTBApP","executionInfo":{"status":"ok","timestamp":1617561300383,"user_tz":-300,"elapsed":88620,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}}},"source":["# this line of code saves the trained model to google drive. You can specify path on local drive.\n","# torch.save(model.state_dict(), \"content/drive/MyDrive/datasets/train/weights.pth\")"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"A0unSa1zjAZK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617561313758,"user_tz":-300,"elapsed":101986,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}},"outputId":"49569881-23fa-48e7-9ae8-b7632e9d0ce7"},"source":["# this loads the saved model from give directory.\n","model.load_state_dict(torch.load('/content/drive/MyDrive/datasets/train/weights.pth'))"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iqxzM1rJ_GrG","executionInfo":{"status":"ok","timestamp":1617561673520,"user_tz":-300,"elapsed":461738,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}},"outputId":"6492faa2-7099-4f67-a684-70190acd4491"},"source":["# this code cell calculates accuracy of test data on the trained model.\n","test_loss = 0\n","correct = 0\n","with torch.no_grad():\n","    for data, target in test_loader:\n","        data, target = Variable(data), Variable(target)\n","        data, target = data.to(device), target.to(device)\n","\n","        data = data.view(-1, 100 * 100*3)\n","        net_out = model(data)\n","        # sum up batch loss\n","        test_loss += criterion(net_out, target).item()\n","        pred = net_out.data.max(1)[1]  # get the index of the max log-probability\n","        correct += pred.eq(target.data).sum()\n","\n","    test_loss /= len(test_loader.dataset)\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","            test_loss, correct, len(test_loader.dataset),\n","            100. * correct / len(test_loader.dataset)))"],"execution_count":15,"outputs":[{"output_type":"stream","text":["\n","Test set: Average loss: 0.0105, Accuracy: 1397/2009 (70%)\n","\n"],"name":"stdout"}]}]}