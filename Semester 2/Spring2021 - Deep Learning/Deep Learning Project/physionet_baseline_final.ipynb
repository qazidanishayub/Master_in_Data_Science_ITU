{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"colab":{"name":"physionet_baseline_final.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"awsWKSa4BA1y"},"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Q2VJU0EuqtR"},"source":["# !pip install wfdb"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yEpfLLYTA8vr"},"source":["import os\n","import numpy as np\n","import pandas as pd\n","from scipy.io import loadmat\n","from scipy.signal import butter, lfilter,resample_poly\n","import sys\n","from sklearn.model_selection import KFold\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.metrics import multilabel_confusion_matrix,classification_report\n","from sklearn.model_selection import train_test_split\n","import gc\n","import matplotlib\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","from pathlib import Path\n","import shutil\n","import math\n","from sklearn.svm import SVC\n","import tensorflow as tf\n","from tensorflow.keras.layers import Conv1D, MaxPool1D, Flatten, Dense, Input, Dropout, LSTM, BatchNormalization,GlobalMaxPool1D,Add, ReLU\n","from tensorflow.keras.layers import Bidirectional\n","#from tensorflow.compat.v1.keras.layers import CuDNNGRU\n","from tensorflow.keras.models import Sequential,Model\n","#from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from wfdb import processing\n","from functools import partial\n","import pickle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nLGp18qUBP0Y","executionInfo":{"status":"ok","timestamp":1627502883021,"user_tz":-300,"elapsed":2894,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}},"outputId":"006ee40d-f9ba-42b1-db8a-64498168e527"},"source":["!pip install wfdb"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: wfdb in /usr/local/lib/python3.7/dist-packages (3.4.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from wfdb) (1.0.1)\n","Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from wfdb) (0.10.0)\n","Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from wfdb) (1.1.5)\n","Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wfdb) (3.2.2)\n","Requirement already satisfied: python-dateutil>=2.4.2 in /usr/local/lib/python3.7/dist-packages (from wfdb) (2.8.1)\n","Requirement already satisfied: certifi>=2016.8.2 in /usr/local/lib/python3.7/dist-packages (from wfdb) (2021.5.30)\n","Requirement already satisfied: pyparsing>=2.0.4 in /usr/local/lib/python3.7/dist-packages (from wfdb) (2.4.7)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from wfdb) (2018.9)\n","Requirement already satisfied: threadpoolctl>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wfdb) (2.2.0)\n","Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from wfdb) (0.22.2.post1)\n","Requirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.7/dist-packages (from wfdb) (2.23.0)\n","Requirement already satisfied: numpy>=1.10.1 in /usr/local/lib/python3.7/dist-packages (from wfdb) (1.19.5)\n","Requirement already satisfied: urllib3>=1.22 in /usr/local/lib/python3.7/dist-packages (from wfdb) (1.24.3)\n","Requirement already satisfied: idna>=2.2 in /usr/local/lib/python3.7/dist-packages (from wfdb) (2.10)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from wfdb) (1.4.1)\n","Requirement already satisfied: chardet>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from wfdb) (3.0.4)\n","Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from wfdb) (1.3.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10.0->wfdb) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AZS_26AcA8vx"},"source":["def evaluate_12ECG_score(label_directory, output_directory):\n","    # Define the weights, the SNOMED CT code for the normal class, and equivalent SNOMED CT codes.\n","    weights_file = 'weights.csv'\n","    normal_class = '426783006'\n","    equivalent_classes = [['713427006', '59118001'], ['284470004', '63593006'], ['427172004', '17338001']]\n","\n","    # Find the label and output files.\n","    print('Finding label and output files...')\n","    label_files, output_files = find_challenge_files(label_directory, output_directory)\n","\n","    # Load the labels and outputs.\n","    print('Loading labels and outputs...')\n","    label_classes, labels = load_labels(label_files, normal_class, equivalent_classes)\n","    output_classes, binary_outputs, scalar_outputs = load_outputs(output_files, normal_class, equivalent_classes)\n","\n","    # Organize/sort the labels and outputs.\n","    print('Organizing labels and outputs...')\n","    classes, labels, binary_outputs, scalar_outputs = organize_labels_outputs(label_classes, output_classes, labels, binary_outputs, scalar_outputs)\n","\n","    # Load the weights for the Challenge metric.\n","    print('Loading weights...')\n","    weights = load_weights(weights_file, classes)\n","\n","    # Only consider classes that are scored with the Challenge metric.\n","    indices = np.any(weights, axis=0) # Find indices of classes in weight matrix.\n","    classes = [x for i, x in enumerate(classes) if indices[i]]\n","    labels = labels[:, indices]\n","    scalar_outputs = scalar_outputs[:, indices]\n","    binary_outputs = binary_outputs[:, indices]\n","    weights = weights[np.ix_(indices, indices)]\n","\n","    # Evaluate the model by comparing the labels and outputs.\n","    print('Evaluating model...')\n","    print('- AUROC and AUPRC...')\n","    auroc, auprc = compute_auc(labels, scalar_outputs)\n","    print('- Accuracy...')\n","    accuracy = compute_accuracy(labels, binary_outputs)\n","    print('- F-measure...')\n","    f_measure = compute_f_measure(labels, binary_outputs)\n","    print('- F-beta and G-beta measures...')\n","    f_beta_measure, g_beta_measure = compute_beta_measures(labels, binary_outputs, beta=2)\n","    print('- Challenge metric...')\n","    challenge_metric = compute_challenge_metric(weights, labels, binary_outputs, classes, normal_class)\n","    print('Done.')\n","\n","    # Return the results.\n","    return auroc, auprc, accuracy, f_measure, f_beta_measure, g_beta_measure, challenge_metric"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z1yuC1itA8vy"},"source":["Check if the input is a number."]},{"cell_type":"code","metadata":{"id":"KXUh26ywA8vz"},"source":["def is_number(x):\n","    try:\n","        float(x)\n","        return True\n","    except ValueError:\n","        return False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wr5nxz1_A8v0"},"source":["Find Challenge files."]},{"cell_type":"code","metadata":{"id":"-_wHuw3LA8v1"},"source":["def find_challenge_files(label_directory, output_directory):\n","    label_files = list()\n","    output_files = list()\n","    for f in sorted(os.listdir(label_directory)):\n","        F = os.path.join(label_directory, f) # Full path for label file\n","        if os.path.isfile(F) and F.lower().endswith('.hea') and not f.lower().startswith('.'):\n","            root, ext = os.path.splitext(f)\n","            g = root + '.csv'\n","            G = os.path.join(output_directory, g) # Full path for corresponding output file\n","            if os.path.isfile(G):\n","                label_files.append(F)\n","                output_files.append(G)\n","            else:\n","                raise IOError('Output file {} not found for label file {}.'.format(g, f))\n","    if label_files and output_files:\n","        return label_files, output_files\n","    else:\n","        raise IOError('No label or output files found.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YgeItR04A8v2"},"source":["Load labels from header/label files."]},{"cell_type":"code","metadata":{"id":"xAaPOQsBA8v3"},"source":["def load_labels(label_files, normal_class, equivalent_classes_collection):\n","    # The labels should have the following form:\n","    #\n","    # Dx: label_1, label_2, label_3\n","    #\n","    num_recordings = len(label_files)\n","\n","    # Load diagnoses.\n","    tmp_labels = list()\n","    for i in range(num_recordings):\n","        with open(label_files[i], 'r') as f:\n","            for l in f:\n","                if l.startswith('#Dx'):\n","                    dxs = set(arr.strip() for arr in l.split(': ')[1].split(','))\n","                    tmp_labels.append(dxs)\n","\n","    # Identify classes.\n","    classes = set.union(*map(set, tmp_labels))\n","    if normal_class not in classes:\n","        classes.add(normal_class)\n","        print('- The normal class {} is not one of the label classes, so it has been automatically added, but please check that you chose the correct normal class.'.format(normal_class))\n","    classes = sorted(classes)\n","    num_classes = len(classes)\n","\n","    # Use one-hot encoding for labels.\n","    labels = np.zeros((num_recordings, num_classes), dtype=np.bool)\n","    for i in range(num_recordings):\n","        dxs = tmp_labels[i]\n","        for dx in dxs:\n","            j = classes.index(dx)\n","            labels[i, j] = 1\n","\n","    # For each set of equivalent class, use only one class as the representative class for the set and discard the other classes in the set.\n","    # The label for the representative class is positive if any of the labels in the set is positive.\n","    remove_classes = list()\n","    remove_indices = list()\n","    for equivalent_classes in equivalent_classes_collection:\n","        equivalent_classes = [x for x in equivalent_classes if x in classes]\n","        if len(equivalent_classes)>1:\n","            representative_class = equivalent_classes[0]\n","            other_classes = equivalent_classes[1:]\n","            equivalent_indices = [classes.index(x) for x in equivalent_classes]\n","            representative_index = equivalent_indices[0]\n","            other_indices = equivalent_indices[1:]\n","            labels[:, representative_index] = np.any(labels[:, equivalent_indices], axis=1)\n","            remove_classes += other_classes\n","            remove_indices += other_indices\n","    for x in remove_classes:\n","        classes.remove(x)\n","    labels = np.delete(labels, remove_indices, axis=1)\n","    return classes, labels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jx_0TYwfA8v6"},"source":["Load outputs from output files."]},{"cell_type":"code","metadata":{"id":"W-TABpx_A8v7"},"source":["def load_outputs(output_files, normal_class, equivalent_classes_collection):\n","    # The outputs should have the following form:\n","    #\n","    # diagnosis_1, diagnosis_2, diagnosis_3\n","    #           0,           1,           1\n","    #        0.12,        0.34,        0.56\n","    #\n","    num_recordings = len(output_files)\n","    tmp_labels = list()\n","    tmp_binary_outputs = list()\n","    tmp_scalar_outputs = list()\n","    for i in range(num_recordings):\n","        with open(output_files[i], 'r') as f:\n","            for j, l in enumerate(f):\n","                arrs = [arr.strip() for arr in l.split(',')]\n","                if j==1:\n","                    row = arrs\n","                    tmp_labels.append(row)\n","                elif j==2:\n","                    row = list()\n","                    for arr in arrs:\n","                        number = 1 if arr in ('1', 'True', 'true', 'T', 't') else 0\n","                        row.append(number)\n","                    tmp_binary_outputs.append(row)\n","                elif j==3:\n","                    row = list()\n","                    for arr in arrs:\n","                        number = float(arr) if is_number(arr) else 0\n","                        row.append(number)\n","                    tmp_scalar_outputs.append(row)\n","\n","    # Identify classes.\n","    classes = set.union(*map(set, tmp_labels))\n","    if normal_class not in classes:\n","        classes.add(normal_class)\n","        print('- The normal class {} is not one of the output classes, so it has been automatically added, but please check that you identified the correct normal class.'.format(normal_class))\n","    classes = sorted(classes)\n","    num_classes = len(classes)\n","\n","    # Use one-hot encoding for binary outputs and the same order for scalar outputs.\n","    binary_outputs = np.zeros((num_recordings, num_classes), dtype=np.bool)\n","    scalar_outputs = np.zeros((num_recordings, num_classes), dtype=np.float64)\n","    for i in range(num_recordings):\n","        dxs = tmp_labels[i]\n","        for k, dx in enumerate(dxs):\n","            j = classes.index(dx)\n","            binary_outputs[i, j] = tmp_binary_outputs[i][k]\n","            scalar_outputs[i, j] = tmp_scalar_outputs[i][k]\n","\n","    # For each set of equivalent class, use only one class as the representative class for the set and discard the other classes in the set.\n","    # The binary output for the representative class is positive if any of the classes in the set is positive.\n","    # The scalar output is the mean of the scalar outputs for the classes in the set.\n","    remove_classes = list()\n","    remove_indices = list()\n","    for equivalent_classes in equivalent_classes_collection:\n","        equivalent_classes = [x for x in equivalent_classes if x in classes]\n","        if len(equivalent_classes)>1:\n","            representative_class = equivalent_classes[0]\n","            other_classes = equivalent_classes[1:]\n","            equivalent_indices = [classes.index(x) for x in equivalent_classes]\n","            representative_index = equivalent_indices[0]\n","            other_indices = equivalent_indices[1:]\n","            binary_outputs[:, representative_index] = np.any(binary_outputs[:, equivalent_indices], axis=1)\n","            scalar_outputs[:, representative_index] = np.nanmean(scalar_outputs[:, equivalent_indices], axis=1)\n","            remove_classes += other_classes\n","            remove_indices += other_indices\n","    for x in remove_classes:\n","        classes.remove(x)\n","    binary_outputs = np.delete(binary_outputs, remove_indices, axis=1)\n","    scalar_outputs = np.delete(scalar_outputs, remove_indices, axis=1)\n","\n","    # If any of the outputs is a NaN, then replace it with a zero.\n","    binary_outputs[np.isnan(binary_outputs)] = 0\n","    scalar_outputs[np.isnan(scalar_outputs)] = 0\n","    return classes, binary_outputs, scalar_outputs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PEspEODxA8v8"},"source":["Organize labels and outputs."]},{"cell_type":"code","metadata":{"id":"EhNQh55TA8v9"},"source":["def organize_labels_outputs(label_classes, output_classes, tmp_labels, tmp_binary_outputs, tmp_scalar_outputs):\n","    # Include all classes from either the labels or the outputs.\n","    classes = sorted(set(label_classes) | set(output_classes))\n","    num_classes = len(classes)\n","\n","    # Check that the labels and outputs have the same numbers of recordings.\n","    assert(len(tmp_labels)==len(tmp_binary_outputs)==len(tmp_scalar_outputs))\n","    num_recordings = len(tmp_labels)\n","\n","    # Rearrange the columns of the labels and the outputs to be consistent with the order of the classes.\n","    labels = np.zeros((num_recordings, num_classes), dtype=np.bool)\n","    for k, dx in enumerate(label_classes):\n","        j = classes.index(dx)\n","        labels[:, j] = tmp_labels[:, k]\n","    binary_outputs = np.zeros((num_recordings, num_classes), dtype=np.bool)\n","    scalar_outputs = np.zeros((num_recordings, num_classes), dtype=np.float64)\n","    for k, dx in enumerate(output_classes):\n","        j = classes.index(dx)\n","        binary_outputs[:, j] = tmp_binary_outputs[:, k]\n","        scalar_outputs[:, j] = tmp_scalar_outputs[:, k]\n","    return classes, labels, binary_outputs, scalar_outputs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VobhD5RMA8v-"},"source":["Load a table with row and column names."]},{"cell_type":"code","metadata":{"id":"Ybtgf_KzA8v_"},"source":["def load_table(table_file):\n","    # The table should have the following form:\n","    #\n","    # ,    a,   b,   c\n","    # a, 1.2, 2.3, 3.4\n","    # b, 4.5, 5.6, 6.7\n","    # c, 7.8, 8.9, 9.0\n","    #\n","    table = list()\n","    with open(table_file, 'r') as f:\n","        for i, l in enumerate(f):\n","            arrs = [arr.strip() for arr in l.split(',')]\n","            table.append(arrs)\n","\n","    # Define the numbers of rows and columns and check for errors.\n","    num_rows = len(table)-1\n","    if num_rows<1:\n","        raise Exception('The table {} is empty.'.format(table_file))\n","    num_cols = set(len(table[i])-1 for i in range(num_rows))\n","    if len(num_cols)!=1:\n","        raise Exception('The table {} has rows with different lengths.'.format(table_file))\n","    num_cols = min(num_cols)\n","    if num_cols<1:\n","        raise Exception('The table {} is empty.'.format(table_file))\n","\n","    # Find the row and column labels.\n","    rows = [table[0][j+1] for j in range(num_rows)]\n","    cols = [table[i+1][0] for i in range(num_cols)]\n","\n","    # Find the entries of the table.\n","    values = np.zeros((num_rows, num_cols))\n","    for i in range(num_rows):\n","        for j in range(num_cols):\n","            value = table[i+1][j+1]\n","            if is_number(value):\n","                values[i, j] = float(value)\n","            else:\n","                values[i, j] = float('nan')\n","    return rows, cols, values"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"31iqEOcKA8wB"},"source":["Load weights."]},{"cell_type":"code","metadata":{"id":"YnHddkb_A8wB"},"source":["def load_weights(weight_file, classes):\n","    # Load the weight matrix.\n","    rows, cols, values = load_table(weight_file)\n","    assert(rows == cols)\n","    num_rows = len(rows)\n","\n","    # Assign the entries of the weight matrix with rows and columns corresponding to the classes.\n","    num_classes = len(classes)\n","    weights = np.zeros((num_classes, num_classes), dtype=np.float64)\n","    for i, a in enumerate(rows):\n","        if a in classes:\n","            k = classes.index(a)\n","            for j, b in enumerate(rows):\n","                if b in classes:\n","                    l = classes.index(b)\n","                    weights[k, l] = values[i, j]\n","    return weights"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tlgXfCGdA8wC"},"source":["Compute recording-wise accuracy."]},{"cell_type":"code","metadata":{"id":"nZU1SAUTA8wC"},"source":["def compute_accuracy(labels, outputs):\n","    num_recordings, num_classes = np.shape(labels)\n","    num_correct_recordings = 0\n","    for i in range(num_recordings):\n","        if np.all(labels[i, :]==outputs[i, :]):\n","            num_correct_recordings += 1\n","    return float(num_correct_recordings) / float(num_recordings)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oKJ1fVjJA8wC"},"source":["Compute confusion matrices."]},{"cell_type":"code","metadata":{"id":"kVbv7TZlA8wD"},"source":["def compute_confusion_matrices(labels, outputs, normalize=False):\n","    # Compute a binary confusion matrix for each class k:\n","    #\n","    #     [TN_k FN_k]\n","    #     [FP_k TP_k]\n","    #\n","    # If the normalize variable is set to true, then normalize the contributions\n","    # to the confusion matrix by the number of labels per recording.\n","    num_recordings, num_classes = np.shape(labels)\n","    if not normalize:\n","        A = np.zeros((num_classes, 2, 2))\n","        for i in range(num_recordings):\n","            for j in range(num_classes):\n","                if labels[i, j]==1 and outputs[i, j]==1: # TP\n","                    A[j, 1, 1] += 1\n","                elif labels[i, j]==0 and outputs[i, j]==1: # FP\n","                    A[j, 1, 0] += 1\n","                elif labels[i, j]==1 and outputs[i, j]==0: # FN\n","                    A[j, 0, 1] += 1\n","                elif labels[i, j]==0 and outputs[i, j]==0: # TN\n","                    A[j, 0, 0] += 1\n","                else: # This condition should not happen.\n","                    raise ValueError('Error in computing the confusion matrix.')\n","    else:\n","        A = np.zeros((num_classes, 2, 2))\n","        for i in range(num_recordings):\n","            normalization = float(max(np.sum(labels[i, :]), 1))\n","            for j in range(num_classes):\n","                if labels[i, j]==1 and outputs[i, j]==1: # TP\n","                    A[j, 1, 1] += 1.0/normalization\n","                elif labels[i, j]==0 and outputs[i, j]==1: # FP\n","                    A[j, 1, 0] += 1.0/normalization\n","                elif labels[i, j]==1 and outputs[i, j]==0: # FN\n","                    A[j, 0, 1] += 1.0/normalization\n","                elif labels[i, j]==0 and outputs[i, j]==0: # TN\n","                    A[j, 0, 0] += 1.0/normalization\n","                else: # This condition should not happen.\n","                    raise ValueError('Error in computing the confusion matrix.')\n","    return A"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6EenkfN0A8wE"},"source":["Compute macro F-measure."]},{"cell_type":"code","metadata":{"id":"gmd1f5eDA8wF"},"source":["def compute_f_measure(labels, outputs):\n","    num_recordings, num_classes = np.shape(labels)\n","    A = compute_confusion_matrices(labels, outputs)\n","    f_measure = np.zeros(num_classes)\n","    for k in range(num_classes):\n","        tp, fp, fn, tn = A[k, 1, 1], A[k, 1, 0], A[k, 0, 1], A[k, 0, 0]\n","        if 2 * tp + fp + fn:\n","            f_measure[k] = float(2 * tp) / float(2 * tp + fp + fn)\n","        else:\n","            f_measure[k] = float('nan')\n","    macro_f_measure = np.nanmean(f_measure)\n","    return macro_f_measure"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BFRWxyztA8wG"},"source":["Compute F-beta and G-beta measures from the unofficial phase of the Challenge."]},{"cell_type":"code","metadata":{"id":"MvzpVBb9A8wG"},"source":["def compute_beta_measures(labels, outputs, beta):\n","    num_recordings, num_classes = np.shape(labels)\n","    A = compute_confusion_matrices(labels, outputs, normalize=True)\n","    f_beta_measure = np.zeros(num_classes)\n","    g_beta_measure = np.zeros(num_classes)\n","    for k in range(num_classes):\n","        tp, fp, fn, tn = A[k, 1, 1], A[k, 1, 0], A[k, 0, 1], A[k, 0, 0]\n","        if (1+beta**2)*tp + fp + beta**2*fn:\n","            f_beta_measure[k] = float((1+beta**2)*tp) / float((1+beta**2)*tp + fp + beta**2*fn)\n","        else:\n","            f_beta_measure[k] = float('nan')\n","        if tp + fp + beta*fn:\n","            g_beta_measure[k] = float(tp) / float(tp + fp + beta*fn)\n","        else:\n","            g_beta_measure[k] = float('nan')\n","    macro_f_beta_measure = np.nanmean(f_beta_measure)\n","    macro_g_beta_measure = np.nanmean(g_beta_measure)\n","    return macro_f_beta_measure, macro_g_beta_measure"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TAK54AbcA8wG"},"source":["Compute macro AUROC and macro AUPRC."]},{"cell_type":"code","metadata":{"id":"KO4ZZUIqA8wH"},"source":["def compute_auc(labels, outputs):\n","    num_recordings, num_classes = np.shape(labels)\n","\n","    # Compute and summarize the confusion matrices for each class across at distinct output values.\n","    auroc = np.zeros(num_classes)\n","    auprc = np.zeros(num_classes)\n","    for k in range(num_classes):\n","        # We only need to compute TPs, FPs, FNs, and TNs at distinct output values.\n","        thresholds = np.unique(outputs[:, k])\n","        thresholds = np.append(thresholds, thresholds[-1]+1)\n","        thresholds = thresholds[::-1]\n","        num_thresholds = len(thresholds)\n","\n","        # Initialize the TPs, FPs, FNs, and TNs.\n","        tp = np.zeros(num_thresholds)\n","        fp = np.zeros(num_thresholds)\n","        fn = np.zeros(num_thresholds)\n","        tn = np.zeros(num_thresholds)\n","        fn[0] = np.sum(labels[:, k]==1)\n","        tn[0] = np.sum(labels[:, k]==0)\n","\n","        # Find the indices that result in sorted output values.\n","        idx = np.argsort(outputs[:, k])[::-1]\n","\n","        # Compute the TPs, FPs, FNs, and TNs for class k across thresholds.\n","        i = 0\n","        for j in range(1, num_thresholds):\n","            # Initialize TPs, FPs, FNs, and TNs using values at previous threshold.\n","            tp[j] = tp[j-1]\n","            fp[j] = fp[j-1]\n","            fn[j] = fn[j-1]\n","            tn[j] = tn[j-1]\n","\n","            # Update the TPs, FPs, FNs, and TNs at i-th output value.\n","            while i < num_recordings and outputs[idx[i], k] >= thresholds[j]:\n","                if labels[idx[i], k]:\n","                    tp[j] += 1\n","                    fn[j] -= 1\n","                else:\n","                    fp[j] += 1\n","                    tn[j] -= 1\n","                i += 1\n","\n","        # Summarize the TPs, FPs, FNs, and TNs for class k.\n","        tpr = np.zeros(num_thresholds)\n","        tnr = np.zeros(num_thresholds)\n","        ppv = np.zeros(num_thresholds)\n","        npv = np.zeros(num_thresholds)\n","        for j in range(num_thresholds):\n","            if tp[j] + fn[j]:\n","                tpr[j] = float(tp[j]) / float(tp[j] + fn[j])\n","            else:\n","                tpr[j] = float('nan')\n","            if fp[j] + tn[j]:\n","                tnr[j] = float(tn[j]) / float(fp[j] + tn[j])\n","            else:\n","                tnr[j] = float('nan')\n","            if tp[j] + fp[j]:\n","                ppv[j] = float(tp[j]) / float(tp[j] + fp[j])\n","            else:\n","                ppv[j] = float('nan')\n","\n","        # Compute AUROC as the area under a piecewise linear function with TPR/\n","        # sensitivity (x-axis) and TNR/specificity (y-axis) and AUPRC as the area\n","        # under a piecewise constant with TPR/recall (x-axis) and PPV/precision\n","        # (y-axis) for class k.\n","        for j in range(num_thresholds-1):\n","            auroc[k] += 0.5 * (tpr[j+1] - tpr[j]) * (tnr[j+1] + tnr[j])\n","            auprc[k] += (tpr[j+1] - tpr[j]) * ppv[j+1]\n","\n","    # Compute macro AUROC and macro AUPRC across classes.\n","    macro_auroc = np.nanmean(auroc)\n","    macro_auprc = np.nanmean(auprc)\n","    return macro_auroc, macro_auprc"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1xZfx5BIA8wH"},"source":["Compute modified confusion matrix for multi-class, multi-label tasks."]},{"cell_type":"code","metadata":{"id":"DvxmXmGAA8wH"},"source":["def compute_modified_confusion_matrix(labels, outputs):\n","    # Compute a binary multi-class, multi-label confusion matrix, where the rows\n","    # are the labels and the columns are the outputs.\n","    num_recordings, num_classes = np.shape(labels)\n","    A = np.zeros((num_classes, num_classes))\n","\n","    # Iterate over all of the recordings.\n","    for i in range(num_recordings):\n","        # Calculate the number of positive labels and/or outputs.\n","        normalization = float(max(np.sum(np.any((labels[i, :], outputs[i, :]), axis=0)), 1))\n","        # Iterate over all of the classes.\n","        for j in range(num_classes):\n","            # Assign full and/or partial credit for each positive class.\n","            if labels[i, j]:\n","                for k in range(num_classes):\n","                    if outputs[i, k]:\n","                        A[j, k] += 1.0/normalization\n","    return A"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-RfU7gfFA8wI"},"source":["Compute the evaluation metric for the Challenge."]},{"cell_type":"code","metadata":{"id":"S0pPKJBTA8wI"},"source":["def compute_challenge_metric(weights, labels, outputs, classes, normal_class):\n","    num_recordings, num_classes = np.shape(labels)\n","    normal_index = classes.index(normal_class)\n","\n","    # Compute the observed score.\n","    A = compute_modified_confusion_matrix(labels, outputs)\n","    observed_score = np.nansum(weights * A)\n","\n","    # Compute the score for the model that always chooses the correct label(s).\n","    correct_outputs = labels\n","    A = compute_modified_confusion_matrix(labels, correct_outputs)\n","    correct_score = np.nansum(weights * A)\n","\n","    # Compute the score for the model that always chooses the normal class.\n","    inactive_outputs = np.zeros((num_recordings, num_classes), dtype=np.bool)\n","    inactive_outputs[:, normal_index] = 1\n","    A = compute_modified_confusion_matrix(labels, inactive_outputs)\n","    inactive_score = np.nansum(weights * A)\n","    if correct_score != inactive_score:\n","        normalized_score = float(observed_score - inactive_score) / float(correct_score - inactive_score)\n","    else:\n","        normalized_score = float('nan')\n","    return normalized_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"biQWW_R9A8wI"},"source":["def bandpass_filter(data, lowcut = 0.001, highcut = 15.0, signal_freq = 500, filter_order = 1):\n","        \"\"\"\n","        Method responsible for creating and applying Butterworth filter.\n","        :param deque data: raw data\n","        :param float lowcut: filter lowcut frequency value\n","        :param float highcut: filter highcut frequency value\n","        :param int signal_freq: signal frequency in samples per second (Hz)\n","        :param int filter_order: filter order\n","        :return array: filtered data\n","        \"\"\"\n","        nyquist_freq = 0.5 * signal_freq\n","        low = lowcut / nyquist_freq\n","        high = highcut / nyquist_freq\n","        b, a = butter(filter_order, [low, high], btype=\"band\")\n","        y = lfilter(b, a, data)\n","        return y\n","    \n","    \n","def load_challenge_data(filename):\n","    x = loadmat(filename)\n","    #print(x)\n","    data = np.asarray(x['val'], dtype=np.float64)\n","    new_file = filename.replace('.mat','.hea')\n","    input_header_file = os.path.join(new_file)\n","    with open(input_header_file,'r') as f:\n","        header_data=f.readlines()\n","    return data, header_data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I_SDGZsBA8wI"},"source":["Find unique true labels"]},{"cell_type":"code","metadata":{"id":"0ZyliRDdA8wJ"},"source":["def get_true_labels(input_file,classes,classes_cases):\n","    classes_label = classes\n","    single_recording_labels=np.zeros(len(classes),dtype=int)\n","    scored_classes_flag=False\n","    with open(input_file,'r') as f:\n","        first_line = f.readline()\n","        recording_label=first_line.split(' ')[0]\n","        #print(recording_label)\n","        for lines in f:\n","            if lines.startswith('#Dx'):\n","                tmp = lines.split(': ')[1].split(',')\n","                for c in tmp:\n","                    current_class=int(c.strip())\n","                    \n","                    if current_class in classes_label:\n","                        scored_classes_flag=True\n","                        idx = classes.index(current_class)\n","                        if classes_cases[idx]>0:\n","                            classes_cases[idx]-=1\n","                            single_recording_labels[idx]=1\n","    return scored_classes_flag,recording_label,classes_label,single_recording_labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mKU_uq_NA8wJ"},"source":["def extend_ts(ts, length):\n","    extended = np.zeros(length)\n","    siglength = np.min([length, ts.shape[0]])\n","    extended[:siglength] = ts[:siglength]\n","    return extended "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ahLSmm3A8wJ"},"source":["def readData(input_files,labels,classes,scored_classes):\n","    fs=500\n","    num_leads = 12\n","    frame_len = 15000\n","    num_classes = len(classes)\n","    \n","    \n","    #__________________________________________________________________\n","    # Data from all mat files.\n","    #__________________________________________________________________\n","    num_files = len(input_files)\n","    multi_labels = []\n","    X = []\n","    y = []\n","    \n","    normalize = partial(processing.normalize_bound, lb=-1, ub=1)\n","    # Iterate over files.\n","    for i, f in enumerate(input_files):\n","        \n","        print('    {}/{}...'.format(i+1, num_files))\n","        # Creating temporary variables for current signal and label\n","        temp_x = np.zeros((1, frame_len, num_leads), dtype = np.float32)\n","        temp_y = np.zeros((1),dtype=int)\n","        multi_labels_temp=np.zeros((num_classes),dtype=int)\n","        # Mat files. (ECG data)\n","        tmp_input_file = f\n","        data,header_data = load_challenge_data(tmp_input_file)\n","        \n","        \n","        #___________________________________________________________________________\n","        # Reading Header data and processing it\n","        #\n","        \n","        # Header files. (ECG Labels)\n","        g = f.replace('.mat','.hea')\n","        tmp_input_file = g\n","        \n","        # Read sampled frequency\n","        with open(tmp_input_file,'r') as f:\n","            first_line = f.readline()\n","            sampled_fs=int(first_line.split(' ')[2])\n","        # If sample frequency is not 500. Resample data\n","        if sampled_fs!=fs:\n","            data=resample_poly(data, fs, sampled_fs,axis=1)\n","            \n","        \n","        #___________________________________________________________________________\n","        # Reading Signal data and processing it\n","        #\n","        \n","        # If length of ecg signal is greater than the frame length just truncate it. \n","        if data.shape[1] > frame_len:\n","            data = data[:,:frame_len]\n","        extended_data = np.zeros((num_leads,frame_len))\n","        for j in range(num_leads):\n","\n","            # If all values in a lead are not zero. \n","            if data[j,:].any():\n","                # Frame Normalization\n","                data[j,:] = np.squeeze(np.apply_along_axis(normalize, 0, data[j,:]))\n","\n","            # padding zeros and bandpass filtering. \n","            extended_data[j,:] = bandpass_filter(extend_ts(data[j,:], length = frame_len))\n","        temp_x = extended_data.T\n","        \n","        #___________________________________________________________________________\n","        # Finalizing Labels and Signals into X and y\n","        #\n","        \n","        # Creating multiple Xs and ys for multi labelled files\n","        temp_y=labels[i]\n","        y.append(temp_y)\n","        X.append(temp_x)\n","        multi_labels_temp[labels[i]]=1\n","        multi_labels.append(multi_labels_temp)\n","        \n","            \n","            \n","    # Collect unused Variables\n","    gc.collect()\n","    del extended_data,input_files,data\n","    \n","    \n","    \n","    # Converting Python lists to final Training Array\n","    X = np.asarray(X, dtype=np.float32)\n","    y = np.asarray(y, dtype=np.int)\n","    multi_labels =  np.asarray(multi_labels, dtype=np.int)\n","    \n","    \n","    \n","    #\n","    # Print Data Stats\n","    #________________________________________________________\n","    \n","    print(\"_______________________PRINTING DATA STATS_________________________________\")\n","    print(\"Classes and their Cases\")\n","    for idx in range(len(classes)):\n","        print(scored_classes.get(classes[idx]),abs(500-classes_cases[idx]))\n","    print(\"Final Data Shape\")\n","    print(X.shape,y.shape)\n","    print('Done.')\n","    #__________________________________________________________________\n","    \n","    \n","    return X,y,multi_labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"649eiO5FA8wK"},"source":["def readFiles(input_directory):\n","\n","    # Getting All folders in Input_directory\n","    folders = [dI for dI in os.listdir(input_directory) if os.path.isdir(os.path.join(input_directory,dI))]\n","    \n","    #__________________________________________________________________\n","    # Find all mat files in data directory. \n","    #__________________________________________________________________\n","    input_files = []\n","    \n","    for folder in folders:\n","        current_folder=os.path.join(input_directory,folder)\n","        for f in os.listdir(current_folder):\n","            if os.path.isfile(os.path.join(current_folder, f)) and not f.lower().startswith('.') and f.lower().endswith('mat'):\n","                input_files.append(os.path.join(current_folder, f))\n","        \n","    return input_files"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rhFNewDoA8wK"},"source":["def readFilesWithLabels(input_directory,classes,classes_cases):\n","\n","    # Getting All folders in Input_directory\n","    folders = [dI for dI in os.listdir(input_directory) if os.path.isdir(os.path.join(input_directory,dI))]\n","    \n","    #__________________________________________________________________\n","    # Find all mat files in data directory. \n","    #__________________________________________________________________\n","    input_files = []\n","    labels=[]\n","    for folder in folders:\n","        \n","        current_folder=os.path.join(input_directory,folder)\n","        for f in os.listdir(current_folder):\n","            if os.path.isfile(os.path.join(current_folder, f)) and not f.lower().startswith('.') and f.lower().endswith('mat'):\n","                \n","                \n","                current_file = os.path.join(current_folder, f)\n","                #___________________________________________________________________________\n","                # Reading Header data and processing it\n","                #\n","                # Header files. (ECG Labels)\n","                g = current_file.replace('.mat','.hea')\n","                tmp_input_file = g\n","                # Check if the current class is in scored classes. Otherwise Skip the file\n","                scored_classes_flag,recording_label,classes_label,multi_labels_temp=get_true_labels(tmp_input_file,classes,classes_cases)\n","                # Skipping the files where no scored class label is found\n","                if not scored_classes_flag:\n","                    # print(\"No Scored Class Found in this file\")\n","                    continue\n","                # Taking All indexes where class label is 1\n","                idx = np.where(multi_labels_temp == 1)\n","                # Creating multiple filename entries and labels for multi labelled files\n","                for i in range(len(idx[0])):\n","                    temp_y=idx[0][i]\n","                    input_files.append(current_file)\n","                    labels.append(temp_y)\n","                    \n","                \n","                \n","    return input_files,labels\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qjx-7HERA8wK"},"source":["def conv_batchnorm_relu(x, filters, kernel_size, strides=1):\n","    x = Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, padding = 'same')(x)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","    return x#Identity block"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K_SO1tWVA8wL"},"source":["def identity_block(tensor, filters):\n","    x = conv_batchnorm_relu(tensor, filters=filters, kernel_size=1, strides=1)\n","    x = conv_batchnorm_relu(x, filters=filters, kernel_size=3, strides=1)\n","    x = Conv1D(filters=4*filters, kernel_size=1, strides=1)(x)\n","    x = BatchNormalization()(x)\n","    x = Add()([tensor,x])    #skip connection\n","    x = ReLU()(x)\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s3X-BGaHA8wL"},"source":["rojection block "]},{"cell_type":"code","metadata":{"id":"KF5jubcaA8wL"},"source":[" \n","def projection_block(tensor, filters, strides): \n","         \n","     #left stream     \n","     x = conv_batchnorm_relu(tensor, filters=filters, kernel_size=1, strides=strides)     \n","     x = conv_batchnorm_relu(x, filters=filters, kernel_size=3, strides=1)     \n","     x = Conv1D(filters=4*filters, kernel_size=1, strides=1)(x)     \n","     x = BatchNormalization()(x) \n","         \n","     #right stream     \n","     shortcut = Conv1D(filters=4*filters, kernel_size=1, strides=strides)(tensor)     \n","     shortcut = BatchNormalization()(shortcut)          \n","     x = Add()([shortcut,x])    #skip connection     \n","     x = ReLU()(x)          \n","     return x#Resnet block"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"58HloLk-A8wL"},"source":["def resnet_block(x, filters, reps, strides):\n","    \n","    x = projection_block(x, filters, strides)\n","    for _ in range(reps-1):\n","        x = identity_block(x,filters)\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H0QaUqdsA8wM"},"source":["def create_model(frame_len,num_classes):\n","    \n","        model = Sequential([   \n","        Input(shape=(frame_len, 12)),\n","        Conv1D(64, 15, activation='relu'),\n","        MaxPool1D(2),\n","        Conv1D(64, 15, activation='relu'),\n","        MaxPool1D(2),\n","        Conv1D(64, 15, activation='relu'),\n","        MaxPool1D(2),\n","        Conv1D(64, 9, activation='relu'),\n","        MaxPool1D(3),\n","        Conv1D(64, 9, activation='relu'),\n","        MaxPool1D(3),\n","        Conv1D(32, 9, activation='relu'),\n","        MaxPool1D(3),\n","        Conv1D(32, 3, activation='relu'),\n","        MaxPool1D(4),\n","        GlobalMaxPool1D(),\n","        Dense(64, kernel_initializer='normal', activation='relu'),\n","        Dense(num_classes,activation='sigmoid', kernel_initializer='normal')])\n","        model.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","        \n","        return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZE1f_Bw9A8wM"},"source":["def create_resnet_model (frame_len,num_classes):\n","    \n","    input = Input(shape=(frame_len, 12))\n","    x = conv_batchnorm_relu(input, filters=64, kernel_size=7, strides=2)\n","    x = MaxPool1D(pool_size = 3, strides =2)(x)\n","    x = resnet_block(x, filters=64, reps =3, strides=1)\n","    x = resnet_block(x, filters=128, reps =4, strides=2)\n","    x = resnet_block(x, filters=256, reps =6, strides=2)\n","    x = resnet_block(x, filters=512, reps =3, strides=2)\n","    x = GlobalMaxPool1D()(x)\n","    x = Dense(64, kernel_initializer='normal', activation='relu')(x)\n","    output = Dense(num_classes, activation ='softmax',kernel_initializer='normal')(x)\n","    model = Model(inputs=input, outputs=output)\n","    model.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dnJvS_ilA8wN"},"source":["def readDataWithoutLabels(input_files):\n","    fs=500\n","    num_leads = 12\n","    frame_len = 15000\n","    \n","    \n","    #__________________________________________________________________\n","    # Data from all mat files.\n","    #__________________________________________________________________\n","    num_files = len(input_files)\n","    X = []\n","    \n","    normalize = partial(processing.normalize_bound, lb=-1, ub=1)\n","    # Iterate over files.\n","    for i, f in enumerate(input_files):\n","        \n","#         print('    {}/{}...'.format(i+1, num_files))\n","        # Creating temporary variables for current signal and label\n","        temp_x = np.zeros((1, frame_len, num_leads), dtype = np.float32)\n","        # Mat files. (ECG data)\n","        tmp_input_file = f\n","        data,header_data = load_challenge_data(tmp_input_file)\n","        \n","        \n","        #___________________________________________________________________________\n","        # Reading Header data and processing it\n","        #\n","        \n","        # Header files. (ECG Labels)\n","        g = f.replace('.mat','.hea')\n","        tmp_input_file = g\n","        \n","        # Read sampled frequency\n","        with open(tmp_input_file,'r') as f:\n","            first_line = f.readline()\n","            sampled_fs=int(first_line.split(' ')[2])\n","        # If sample frequency is not 500. Resample data\n","        if sampled_fs!=fs:\n","            data=resample_poly(data, fs, sampled_fs,axis=1)\n","            \n","        \n","        #___________________________________________________________________________\n","        # Reading Signal data and processing it\n","        #\n","        \n","        # If length of ecg signal is greater than the frame length just truncate it. \n","        if data.shape[1] > frame_len:\n","            data = data[:,:frame_len]\n","        extended_data = np.zeros((num_leads,frame_len))\n","        for j in range(num_leads):\n","\n","            # If all values in a lead are not zero. \n","            if data[j,:].any():\n","                # Frame Normalization\n","                data[j,:] = np.squeeze(np.apply_along_axis(normalize, 0, data[j,:]))\n","\n","            # padding zeros and bandpass filtering. \n","            extended_data[j,:] = bandpass_filter(extend_ts(data[j,:], length = frame_len))\n","        temp_x = extended_data.T\n","        \n","        #___________________________________________________________________________\n","        # Finalizing Labels and Signals into X and y\n","        #\n","        \n","        # Creating multiple Xs and ys for multi labelled files\n","        X.append(temp_x)\n","    \n","    return X"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vhLonwzGA8wN"},"source":["def save_challenge_predictions(output_directory,filenames,scores,labels,classes):\n","    \n","    for idx,file in enumerate(filenames):\n","        \n","        filename = os.path.basename(file)\n","        recording = os.path.splitext(filename)[0]\n","        new_file = filename.replace('.mat','.csv')\n","        output_file = os.path.join(output_directory,new_file)\n","\n","        # Include the filename as the recording number\n","        recording_string = '#{}'.format(recording)\n","        class_string = ','.join(map(str, classes))\n","        label_string = ','.join(str(i) for i in labels[idx])\n","        score_string = ','.join(str(i) for i in scores[idx])\n","        with open(output_file, 'w') as f:\n","            f.write(recording_string + '\\n' + class_string + '\\n' + label_string + '\\n' + score_string + '\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3hPPdJ24A8wN"},"source":["def save_original_labels(label_directory,filenames):\n","    for file in filenames:\n","        \n","        filename = os.path.basename(file)\n","        label_file=os.path.join(label_directory,filename.replace('.mat','.hea'))\n","        shutil.copyfile(file.replace('.mat','.hea'), label_file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V-6pROn-A8wN"},"source":["def equvialentClassesConversion(scored_classes,classes,labels):\n","    equivalent_classes_collection = [[713427006, 59118001], [284470004, 63593006], [427172004, 17338001]]\n","    # For each set of equivalent class, use only one class as the representative class for the set and discard the other classes in the set.\n","    # The label for the representative class is positive if any of the labels in the set is positive.\n","    remove_classes = list()\n","    remove_indices = list()\n","    for equivalent_classes in equivalent_classes_collection:\n","        equivalent_classes = [x for x in equivalent_classes if x in classes]\n","        if len(equivalent_classes)>1:\n","            representative_class = equivalent_classes[0]\n","            other_classes = equivalent_classes[1:]\n","            equivalent_indices = [classes.index(x) for x in equivalent_classes]\n","            representative_index = equivalent_indices[0]\n","            other_indices = equivalent_indices[1:]\n","            labels[:, representative_index] = np.any(labels[:, equivalent_indices], axis=1)\n","            remove_classes += other_classes\n","            remove_indices += other_indices\n","    for x in remove_classes:\n","        classes.remove(x)\n","        del scored_classes[x]\n","    labels = np.delete(labels, remove_indices, axis=1)\n","    return scored_classes,classes, labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E7SQXrFKA8wO"},"source":["def createBinarySVC(classes,max_cases_svc):\n","    max_cases=max_cases_svc\n","    for idx,current_class in enumerate(classes):\n","        if current_class==713427006:\n","            current_classes=[713427006, 59118001]\n","        elif current_class==284470004:\n","            current_classes=[284470004, 63593006]\n","        elif current_class==427172004:\n","            current_classes=[427172004, 17338001]\n","        else:\n","            current_classes=[current_class]\n","        # Remainder Classes\n","        remainder_classes=classes.copy()\n","        remainder_classes.remove(current_class)\n","        # Read All files with labels of current clas\n","        current_class_cases=[max_cases]*len(current_classes)\n","        input_files_current , _ = readFilesWithLabels(input_directory='data',classes=current_classes,classes_cases=current_class_cases)\n","        print('input_files_current',len(input_files_current))\n","        # Claculate how many cases of current class\n","        current_class_cases=len(input_files_current)\n","        # Read All files with labels of remainder classes\n","        remainder_cases=[math.ceil(current_class_cases/(len(classes)-1))]*(len(classes)-1)\n","        input_files_remainder, _ =readFilesWithLabels(input_directory='data',classes=remainder_classes,classes_cases=remainder_cases)\n","        print('input_files_remainder:',len(input_files_remainder))\n","        \n","        #Read data from files\n","        X_current=readDataWithoutLabels(input_files_current)\n","        y_current=[1]*len(X_current)\n","        X_remainder=readDataWithoutLabels(input_files_remainder)\n","        y_remainder=[0]*len(X_remainder)\n","        X = X_current + X_remainder\n","        y = y_current + y_remainder\n","        # Converting Python lists to final Training Array\n","        X = np.asarray(X, dtype=np.float32)\n","        y = np.asarray(y, dtype=np.int)\n","        \n","        print(X.shape,y.shape)\n","       \n","        \n","        #Split data into training and testing\n","        X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y, test_size=0.2, random_state=42)\n","        \n","        #Get Features from feature Extractor\n","        feat_train = model_feat.predict(X_train,batch_size=bs)\n","        feat_test = model_feat.predict(X_test,batch_size=bs)\n","        \n","        #Train SVM\n","        print('Training SVM for Class',current_class)\n","        svm = SVC(kernel='linear')\n","        svm.probability=True\n","        svm.fit(feat_train,y_train)\n","        print('fitting done !!!')\n","        print('Train Score:',svm.score(feat_train,y_train))\n","        print('Test Score:',svm.score(feat_test,y_test))\n","        \n","        # save the model to disk\n","        filename = 'svc_'+str(current_class)+'.sav'\n","        pickle.dump(svm, open(filename, 'wb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KkHP09riB2uH","executionInfo":{"status":"ok","timestamp":1627502885408,"user_tz":-300,"elapsed":62,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}},"outputId":"5d64f775-9b47-41f3-a7fb-acec90c156f8"},"source":["cd /content/drive/MyDrive/MSDS-20/semester 2/Spring2021 - Deep Learning/Deep Learning Project"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/MSDS-20/semester 2/Spring2021 - Deep Learning/Deep Learning Project\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jnGEM4vCvQ3E","executionInfo":{"status":"ok","timestamp":1627503018010,"user_tz":-300,"elapsed":360,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}},"outputId":"3f5c8031-966f-471d-910e-1502feabec24"},"source":["ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["'12-lead ECG Classification Using Deep Neural Networks (2).pptx'\n"," \u001b[0m\u001b[01;34mdata\u001b[0m/\n"," physionet_baseline_final.ipynb\n"," physionet_baseline_final.py\n"," \u001b[01;34mresults\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"C6mvbqXOA8wO"},"source":["if __name__ == '__main__':\n","\n","    # Model Parameters\n","    bs = 12 #Batch size\n","    ep = 2 #epochs\n","    threshold = 0.5 #Threshold\n","    frame_len = 15000 #Frame Length\n","    max_cases = 10 # Max cases for each class for CNN\n","    max_cases_svc = 10 # Max cases for each class for SVC\n","    \n","    #__________________________________________________________________\n","    # All scored Classes \n","    #__________________________________________________________________\n","    scored_classes = {270492004:\"IAVB\",\n","                    164889003:\"AF\",\n","                    164890007:\"AFL\",\n","                    426627000:\"Brady\",\n","                    713427006:\"CRBBB\",\n","                    713426002:\"IRBBB\",\n","                    445118002:\"LAnFB\",\n","                    39732003:\"LAD\",\n","                    164909002:\"LBBB\",\n","                    251146004:\"LQRSV\",\n","                    698252002:\"NSIVCB\",\n","                    10370003:\"PR\",\n","                    284470004:\"PAC\",\n","                    427172004:\"PVC\",\n","                    164947007:\"LPR\",\n","                    111975006:\"LQT\",\n","                    164917005:\"QAb\",\n","                    47665007:\"RAD\",\n","                    59118001:\"RBBB\",\n","                    427393009:\"SA\",\n","                    426177001:\"SB\",\n","                    426783006:\"SNR\",\n","                    427084000:\"STach\",\n","                    63593006:\"SVPB\",\n","                    164934002:\"TAb\",\n","                    59931005:\"TInv\",\n","                    17338001:\"VPB\"\n","                    }\n","    classes = sorted(scored_classes)\n","    # Define how many maximum cases for each class you want\n","    classes_cases = [max_cases] * len(classes)\n","    \n","    \n","    # Read All files with labels according to need as defined by classes_cases\n","    input_files, labels = readFilesWithLabels(input_directory = 'data', classes = classes, classes_cases = classes_cases)\n","    \n","    \n","    #Read data from files\n","    X, y, multi_labels = readData(input_files, labels, classes, scored_classes)\n","    \n","    # Creating One Hot encoding scheme for given classes (27)\n","    n_features = 1\n","    n_labels = len(classes)\n","    categories = [range(n_labels)] * n_features\n","    onehot_encoder = OneHotEncoder(categories = categories, sparse = False)\n","\n","    # Encoding labels\n","    y = onehot_encoder.fit_transform(y.reshape(-1, 1))\n","\n","    # Convert Equivalent Classes Labels\n","    scored_classes, classes, y = equvialentClassesConversion(scored_classes, classes, y)\n","\n","    # Creating One Hot encoding scheme for new classes (24)\n","    n_features = 1\n","    n_labels = len(classes)\n","    categories = [ range( n_labels ) ] * n_features\n","    onehot_encoder = OneHotEncoder (categories=categories,sparse=False)\n","\n","    #Split data into training and testing\n","    X_train, X_test, y_train, y_test, _ ,multi_labels_test, _ ,input_files_test = train_test_split(X, y,multi_labels,input_files,stratify=y, test_size=0.2, random_state=42)\n","    \n","    #gc.collect()\n","    del X, y, multi_labels, input_files\n","\n","    # Create and Manage Directories for Results\n","    label_directory = os.path.join(os.getcwd(),\"outputs\", \"labels\")\n","    cnn_directory = os.path.join(os.getcwd(),\"outputs\", \"cnn\")\n","    shutil.rmtree(label_directory, ignore_errors = True)\n","    shutil.rmtree(cnn_directory, ignore_errors = True)\n","    Path(label_directory).mkdir(parents = True, exist_ok = True)\n","    Path(cnn_directory).mkdir(parents = True, exist_ok = True)\n","    \n","    # Save Test Labels Seperately for score Calculation\n","    save_original_labels(label_directory = label_directory, filenames = input_files_test)\n","\n","    # Create Stats\n","    stats_new=np.zeros((1,7))\n","    \n","    #__________________________________________________________________\n","    # Create Models\n","    #__________________________________________________________________\n","    \n","    # Define CNN model architecture\n","    model_cnn=create_model(frame_len,len(classes))\n","    # model_cnn=create_resnet_model(frame_len,len(classes))\n","    model_path_cnn = 'cnn_model.h5'\n","    checkpoint_cnn = ModelCheckpoint(model_path_cnn, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","    \n","    \n","    print('------------------------------------------------------------------------')\n","    print('Training CNN Model...')\n","    try:\n","        model_cnn.load_weights(model_path_cnn)\n","    except:\n","        pass\n","    history=model_cnn.fit(X_train, y_train,batch_size=bs, epochs=ep,\n","                        validation_data=(X_test, y_test),\n","                        callbacks=[checkpoint_cnn])  # starts training\n","        \n","       "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4BOyPKkYIFJE","executionInfo":{"status":"ok","timestamp":1627502918318,"user_tz":-300,"elapsed":344,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}},"outputId":"9582661f-e2ab-45b7-9bd6-76d23e107541"},"source":["ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["'12-lead ECG Classification Using Deep Neural Networks (2).pptx'\n"," \u001b[0m\u001b[01;34mdata\u001b[0m/\n"," physionet_baseline_final.ipynb\n"," physionet_baseline_final.py\n"," \u001b[01;34mresults\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1jZF7qMeIG6q","executionInfo":{"status":"ok","timestamp":1627502918656,"user_tz":-300,"elapsed":4,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}},"outputId":"2475b58e-b701-44e5-b93c-9810121dfa5e"},"source":["cd data"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/MSDS-20/semester 2/Spring2021 - Deep Learning/Deep Learning Project/data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IuNVwe6xIJEz","executionInfo":{"status":"ok","timestamp":1627502921098,"user_tz":-300,"elapsed":945,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}},"outputId":"728a789c-f0ca-4d81-e916-76eb8aedc3c7"},"source":["!wget -O PhysioNetChallenge2020_Training_CPSC.tar.gz \\\n","https://cloudypipeline.com:9555/api/download/physionet2020training/PhysioNetChallenge2020_Training_CPSC.tar.gz/\n","!wget -O PhysioNetChallenge2020_Training_2.tar.gz \\\n","https://cloudypipeline.com:9555/api/download/physionet2020training/PhysioNetChallenge2020_Training_2.tar.gz/\n","!wget -O PhysioNetChallenge2020_Training_StPetersburg.tar.gz \\\n","https://cloudypipeline.com:9555/api/download/physionet2020training/PhysioNetChallenge2020_Training_StPetersburg.tar.gz/\n","!wget -O PhysioNetChallenge2020_Training_PTB.tar.gz \\\n","https://cloudypipeline.com:9555/api/download/physionet2020training/PhysioNetChallenge2020_Training_PTB.tar.gz/\n","!wget -O PhysioNetChallenge2020_Training_PTB-XL.tar.gz \\\n","https://cloudypipeline.com:9555/api/download/physionet2020training/PhysioNetChallenge2020_PTB-XL.tar.gz/\n","!wget -O PhysioNetChallenge2020_Training_E.tar.gz \\\n","https://cloudypipeline.com:9555/api/download/physionet2020training/PhysioNetChallenge2020_Training_E.tar.gz/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-07-28 20:08:40--  https://cloudypipeline.com:9555/api/download/physionet2020training/PhysioNetChallenge2020_Training_CPSC.tar.gz/\n","Resolving cloudypipeline.com (cloudypipeline.com)... failed: Name or service not known.\n","wget: unable to resolve host address cloudypipeline.com\n","--2021-07-28 20:08:40--  https://cloudypipeline.com:9555/api/download/physionet2020training/PhysioNetChallenge2020_Training_2.tar.gz/\n","Resolving cloudypipeline.com (cloudypipeline.com)... failed: Name or service not known.\n","wget: unable to resolve host address cloudypipeline.com\n","--2021-07-28 20:08:40--  https://cloudypipeline.com:9555/api/download/physionet2020training/PhysioNetChallenge2020_Training_StPetersburg.tar.gz/\n","Resolving cloudypipeline.com (cloudypipeline.com)... failed: Name or service not known.\n","wget: unable to resolve host address cloudypipeline.com\n","--2021-07-28 20:08:40--  https://cloudypipeline.com:9555/api/download/physionet2020training/PhysioNetChallenge2020_Training_PTB.tar.gz/\n","Resolving cloudypipeline.com (cloudypipeline.com)... failed: Name or service not known.\n","wget: unable to resolve host address cloudypipeline.com\n","--2021-07-28 20:08:40--  https://cloudypipeline.com:9555/api/download/physionet2020training/PhysioNetChallenge2020_PTB-XL.tar.gz/\n","Resolving cloudypipeline.com (cloudypipeline.com)... failed: Name or service not known.\n","wget: unable to resolve host address cloudypipeline.com\n","--2021-07-28 20:08:40--  https://cloudypipeline.com:9555/api/download/physionet2020training/PhysioNetChallenge2020_Training_E.tar.gz/\n","Resolving cloudypipeline.com (cloudypipeline.com)... failed: Name or service not known.\n","wget: unable to resolve host address cloudypipeline.com\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DykSkRRCIaM1"},"source":["ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kWgN6n49Ibpi"},"source":["def extract_all(archives, extract_path):\n","    for filename in archives:\n","        shutil.unpack_archive(filename, extract_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":171},"id":"FeWFeGAaQvP2","executionInfo":{"status":"error","timestamp":1627502926307,"user_tz":-300,"elapsed":11,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}},"outputId":"c1b17ad1-d004-4997-e02d-983050081145"},"source":["extract_all(PhysioNetChallenge2020_Training_2.tar.gz, )"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-47-21347e818586>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mextract_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPhysioNetChallenge2020_Training_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'PhysioNetChallenge2020_Training_2' is not defined"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":244},"id":"ZXTlGeu3A8wP","executionInfo":{"status":"error","timestamp":1627502929670,"user_tz":-300,"elapsed":18,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}},"outputId":"2fec27e7-70ce-43e9-9f7a-828000ccdddf"},"source":["    accuracy = history.history['accuracy']\n","    val_accuracy = history.history['val_accuracy']\n","    loss = history.history['loss']\n","    val_loss = history.history['val_loss']\n","    epochs = range(len(accuracy))\n","    fig=plt.figure()\n","    plt.plot(epochs, accuracy,  label='Training accuracy')\n","    plt.plot(epochs, val_accuracy,  label='Validation accuracy')\n","    plt.title('Training and validation accuracy')\n","    plt.legend()\n","    plt.savefig('accuracy_cnn.png')\n","    plt.figure()\n","    plt.plot(epochs, loss,label='Training loss')\n","    plt.plot(epochs, val_loss,  label='Validation loss')\n","    plt.title('Training and validation loss')\n","    plt.legend()\n","    plt.savefig('loss_cnn.png')\n","    plt.close(fig)\n","    \n","    \n","    del X_train,y_train, multi_labels_test\n","     # Load best epoch weights. \n","    model_cnn.load_weights(model_path_cnn)\n","    print('------------------------------------------------------------------------')\n","    print('Define Feature Extractor and create Binary SVCs')\n","    model_feat = Model(inputs=model_cnn.input,outputs=model_cnn.get_layer('global_max_pooling1d').output)\n","    createBinarySVC(classes,max_cases_svc)\n","    print('------------------------------------------------------------------------')\n","    print('Testing Through SVC')\n","\n","    #Get Features from feature Extractor\n","    feat_test = model_feat.predict(X_test,batch_size=bs)\n","    y_test_pred=[]\n","    y_test_pred_score=[]\n","    for idx,current_class in enumerate(classes):\n","        \n","        # load the model from disk\n","        filename='svc_'+str(current_class)+'.sav'\n","        loaded_model = pickle.load(open(filename, 'rb'))\n","        # Make predictions\n","        y_test_pred.append(loaded_model.predict(feat_test))\n","        y_test_pred_score.append(loaded_model.predict_proba(feat_test)[:,-1])\n","    \n","    y_test_pred = np.asarray(y_test_pred, dtype=np.int).T\n","    y_test_pred_score = np.asarray(y_test_pred_score, dtype=np.float32).T    \n","    \n","    \n","    print(classification_report(np.argmax(y_test,1),np.argmax(y_test_pred_score,1),target_names=sorted(scored_classes)))\n","    print(max(val_accuracy))\n","\n","    #Save Model Predictions\n","    save_challenge_predictions(output_directory=cnn_directory,filenames=input_files_test,scores=y_test_pred_score,labels=y_test_pred,classes=sorted(scored_classes))\n","\n","    #Compute new Scores \n","    auroc, auprc, accuracy1, f_measure1, f_beta_measure1, g_beta_measure1, challenge_metric=evaluate_12ECG_score(label_directory=label_directory, output_directory=cnn_directory)\n","    stats_new[0]=[auroc, auprc, accuracy1, f_measure1, f_beta_measure1, g_beta_measure1, challenge_metric]\n","    \n","    # Collect unused variables and delete unused variables\n","    gc.collect()\n","    del X_test,y_test\n","    "],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-48-f16344ebe7d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":262},"id":"eBpxm_8JA8wP","executionInfo":{"status":"error","timestamp":1627502930891,"user_tz":-300,"elapsed":580,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}},"outputId":"34ba288a-3465-469f-bb63-da820a26a85a"},"source":["    # Save stats\n","    df = pd.DataFrame(stats_new,columns = [\"auroc\", \"auprc\", \"accuracy\", \"f_measure\", \"f_beta_measure\", \"g_beta_measure\", \"challenge_metric\"],index=['CNN'])\n","    df.to_csv('stats_new.csv')\n","   \n","    \n","    "],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-49-1349383e1354>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats_new\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"auroc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"auprc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"f_measure\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"f_beta_measure\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"g_beta_measure\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"challenge_metric\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CNN'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stats_new.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'stats_new' is not defined"]}]},{"cell_type":"code","metadata":{"id":"CPEkV-xzu9v0"},"source":[""],"execution_count":null,"outputs":[]}]}