{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment 5 Starter Code.ipynb","provenance":[],"mount_file_id":"1q5H32oD49JlHgBFphkt5SjshJjnMXTUN","authorship_tag":"ABX9TyOj8XvWdunecSrzCr+r5Hzh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r8BDMiSOWieo","executionInfo":{"status":"ok","timestamp":1626787487870,"user_tz":-300,"elapsed":600,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}},"outputId":"2101c38d-e5c7-4c90-b85d-6ffbf9e1d7a5"},"source":["cd /content/drive/MyDrive/MSDS-20/semester 2/Spring2021 - Deep Learning/Deep learning Assignment 5"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/MSDS-20/semester 2/Spring2021 - Deep Learning/Deep learning Assignment 5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XTuP3iXCXYMV","executionInfo":{"status":"ok","timestamp":1626787488339,"user_tz":-300,"elapsed":15,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}},"outputId":"1f893a8c-7961-44e9-869b-7d0833f290d1"},"source":["ls"],"execution_count":2,"outputs":[{"output_type":"stream","text":[" Assignment_5.ipynb                'DL_Assignment 5.pdf'   Report.docx\n","'Assignment 5 Starter Code.ipynb'   encodings.pickle       tokenizer.pickle\n","'DL A5.ipynb'                       imdb_dataset.csv       unlabeled.csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9N46VNSrXpRk","executionInfo":{"status":"ok","timestamp":1626787496367,"user_tz":-300,"elapsed":8033,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}},"outputId":"5ff4f774-4b56-4bee-f465-72d8ab2c4f10"},"source":["pip install pickle5"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting pickle5\n","  Downloading pickle5-0.0.11.tar.gz (132 kB)\n","\u001b[?25l\r\u001b[K     |██▌                             | 10 kB 21.3 MB/s eta 0:00:01\r\u001b[K     |█████                           | 20 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 30 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 40 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 51 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 61 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 71 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 81 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 92 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 132 kB 5.2 MB/s \n","\u001b[?25hBuilding wheels for collected packages: pickle5\n","  Building wheel for pickle5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pickle5: filename=pickle5-0.0.11-cp37-cp37m-linux_x86_64.whl size=219328 sha256=7c5dfd7d6125042c9f462b9f9bd48197dfdf9eed5a57dbd0e8762f3760b06cd7\n","  Stored in directory: /root/.cache/pip/wheels/7e/6a/00/67136a90d6aca437d806d1d3cedf98106e840c97a3e5188198\n","Successfully built pickle5\n","Installing collected packages: pickle5\n","Successfully installed pickle5-0.0.11\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GoWYBII1WX93"},"source":["### Data Preprocessing \n","You can use your own way of preprocessing to enhance results. Best results will lead to bonus points."]},{"cell_type":"code","metadata":{"id":"SXsbb4gZWX-D","executionInfo":{"status":"ok","timestamp":1626787503598,"user_tz":-300,"elapsed":7249,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}}},"source":["import pandas as pd\n","import nltk\n","import tensorflow as tf\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import TensorDataset\n","from torch.autograd import Variable\n","import pickle5 as pickle"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"jECO9cpPWX-G","executionInfo":{"status":"ok","timestamp":1626787505875,"user_tz":-300,"elapsed":2298,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}}},"source":["reviews = pd.read_csv('imdb_dataset.csv')"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"C9OJJSp3WX-I","colab":{"base_uri":"https://localhost:8080/","height":362},"executionInfo":{"status":"ok","timestamp":1626787505880,"user_tz":-300,"elapsed":42,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}},"outputId":"8ce02f2f-4c8c-46dd-a91f-fb90814b9e62"},"source":["reviews.head(10)"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>One of the other reviewers has mentioned that ...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I thought this was a wonderful way to spend ti...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Basically there's a family where a little boy ...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Probably my all-time favorite movie, a story o...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>I sure would like to see a resurrection of a u...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>This show was an amazing, fresh &amp; innovative i...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Encouraged by the positive comments about this...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>If you like original gut wrenching laughter yo...</td>\n","      <td>positive</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              review sentiment\n","0  One of the other reviewers has mentioned that ...  positive\n","1  A wonderful little production. <br /><br />The...  positive\n","2  I thought this was a wonderful way to spend ti...  positive\n","3  Basically there's a family where a little boy ...  negative\n","4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n","5  Probably my all-time favorite movie, a story o...  positive\n","6  I sure would like to see a resurrection of a u...  positive\n","7  This show was an amazing, fresh & innovative i...  negative\n","8  Encouraged by the positive comments about this...  negative\n","9  If you like original gut wrenching laughter yo...  positive"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IP5suMNNgwLa","executionInfo":{"status":"ok","timestamp":1626787505884,"user_tz":-300,"elapsed":28,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}},"outputId":"34068e64-eb29-4df2-b71f-251f775fe12a"},"source":["reviews['sentiment'].value_counts()"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["negative    24000\n","positive    24000\n","Name: sentiment, dtype: int64"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"tDqkAHi7WX-M","executionInfo":{"status":"ok","timestamp":1626708175875,"user_tz":-300,"elapsed":26,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}}},"source":["def preprocess(text):\n","    lower = text.lower()\n","    # Removing Punctuation marks\n","    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n","    rem_punc = tokenizer.tokenize(lower)\n","    # Removing Stop Words\n","    stopwords = nltk.corpus.stopwords.words('english')\n","    rem_stop_words = [word for word in rem_punc if not word in stopwords]\n","    # Removing Non-English words \n","    english_words = nltk.corpus.words.words()\n","    english_words = [word for word in rem_stop_words if word in english_words]   \n","    # Insert Start End tokens\n","    english_words.insert(0,'<start>')\n","    english_words.append('<end>')\n","    sentence = ' '.join(english_words)\n","    return sentence"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bp6jayukWX-Q","executionInfo":{"status":"ok","timestamp":1626708177882,"user_tz":-300,"elapsed":18,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}}},"source":["def encode_text(text):\n","    # Tokenization\n","    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n","    tokenizer.fit_on_texts(text)\n","    # Converting to sequences\n","    sequences = tokenizer.texts_to_sequences(text)\n","    # Padding Zeros \n","    tokenizer.word_index['<pad>'] = 0\n","    tokenizer.index_word[0] = '<pad>'\n","    padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding='post')\n","    \n","    return padded_sequences, tokenizer"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"O49UBVGjWX-R","executionInfo":{"status":"ok","timestamp":1626708177883,"user_tz":-300,"elapsed":17,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}}},"source":["# text = list(map(preprocess,reviews.review))\n","# encodings, tokenizer = encode_text(text)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"jqbDsTblWX-S","executionInfo":{"status":"ok","timestamp":1626708177884,"user_tz":-300,"elapsed":17,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}}},"source":["\n","# with open('encodings.pickle', 'wb') as handle:\n","#     pickle.dump(encodings, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","# with open('tokenizer.pickle', 'wb') as handle:\n","#     pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"hjMbAFXHWX-U","executionInfo":{"status":"ok","timestamp":1626708317509,"user_tz":-300,"elapsed":586,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}}},"source":["with open('tokenizer.pickle', 'rb') as handle:\n","    tokenizer = pickle.load(handle)\n","with open('encodings.pickle', 'rb') as handle:\n","    encodings = pickle.load(handle)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"vAEhWHMXWX-V","executionInfo":{"status":"ok","timestamp":1626708317512,"user_tz":-300,"elapsed":12,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}}},"source":["X = encodings\n","y = np.where(np.array(reviews.sentiment)=='positive',1,0)"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"WvRhbfNlWX-X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626708320122,"user_tz":-300,"elapsed":26,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}},"outputId":"3c37aad8-cead-4db3-88ed-a7ac9fae8b5f"},"source":["X.shape,y.shape"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((48000, 1053), (48000,))"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"R2kmg6lfWX-Y","executionInfo":{"status":"ok","timestamp":1626708320650,"user_tz":-300,"elapsed":12,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}}},"source":["def makeDataloaders(X,y,batch_size):\n","    '''\n","    This is a function that loads the dataset based on the path and sizes for train,test and val sets. \n","    It returns dataloaders for training,testing and validation\n","    '''\n","    \n","\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n","    # X_train = X_train.astype(float)\n","    # X_test = X_test.astype(float)\n","    \n","    train_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n","    test_dataset = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n","    \n","    train_loader=torch.utils.data.DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n","\n","    test_loader=torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n","\n","\n","    return train_loader,test_loader"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"THyFQ4P4WX-Y"},"source":["### Sequence models\n","Following is an example code for simple LSTMs containing one layer. Your implementation should be generic in which user can be able to create multiple layers if required. "]},{"cell_type":"code","metadata":{"id":"rwKTNoCCWX-Z","executionInfo":{"status":"ok","timestamp":1626708321073,"user_tz":-300,"elapsed":17,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}}},"source":["class LSTM(nn.Module):\n","    \n","    def __init__(self,input_dim,n_layers, hidden_dim):\n","\n","        super().__init__()\n","        # use pytorch tensors\n","        # require_grad parameter should be True \n","        # Instead of following you can also use nn.Linear()\n","        self.input_dim  = input_dim\n","        self.hidden_dim = hidden_dim\n","        self.n_layers = n_layers\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, 2)\n","\n","    \n","\n","    def forward(self, inp):\n","\n","\n","        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        # Initialize hidden state with zeros (batch_size, hidden_dim)\n","        h0 = torch.zeros(self.n_layers, inp.size(0), self.hidden_dim).float().requires_grad_()\n","        \n","        # Initialize cell state with zeros (batch_size, hidden_dim)\n","        c0 = torch.zeros(self.n_layers, inp.size(0), self.hidden_dim).float().requires_grad_()\n","        h0, c0 = h0.to(device), c0.to(device)\n","        # Loop through the whole sequence and update h_t and c_t at every time step\n","        # Shape of x is (batch_size, dim)\n","        # out, hidden = self.lstm(inp, (h.detach(), c.detach()))\n","        out, hidden = self.lstm(inp,(h0,c0))\n","        # out , hidden  = out.to(device),hidden.to(device)\n","        # print(hidden.shape)\n","        out =  self.fc(out.squeeze(1))\n","        \n","        return out"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"mSWORrhEWX-c","executionInfo":{"status":"ok","timestamp":1626708321074,"user_tz":-300,"elapsed":16,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}}},"source":["def train(model, input_dim,train_loader, val_loader, training_epochs, loss_func, optimizer):\n","    '''\n","    This is a function to train the model based on provided parameters\n","    It outputs train and val accuracy and loss on each epoch and outputs the trained model\n","    '''\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    train_loss_array,train_acc_array = [], []\n","    val_loss_array,val_acc_array = [], []\n","    for epoch in range(training_epochs):\n","        # Training Phase\n","        correct = 0\n","        for batch_idx, (data,target) in enumerate(train_loader):\n","            # print(batch[0].shape)\n","            data, target = Variable(data), Variable(target)\n","            data, target = data.to(device), target.to(device)\n","            model = model.to(device)\n","            data = data.view(-1, 1, input_dim).float().requires_grad_()\n","            optimizer.zero_grad()\n","            net_out = model(data)\n","            pred = net_out.data.max(1)[1]  # get the index of the max log-probability\n","            \n","            correct += pred.eq(target.data).sum()\n","            loss = loss_func(net_out, target)\n","            # train_loss_array.append(loss)\n","            loss.backward()\n","            optimizer.step()\n","            \n","        train_acc_array.append(correct / len(train_loader.dataset))\n","        train_loss_array.append(loss.item())\n","        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","                epoch, \n","                batch_idx * len(data), \n","                len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), \n","                loss.item(),\n","                correct, \n","                len(train_loader.dataset),\n","                100. * correct / len(train_loader.dataset)))\n","                            \n","        # Validation Phase\n","        val_loss = 0\n","        correct = 0\n","        with torch.no_grad():\n","            for data, target in val_loader:\n","                data, target = Variable(data), Variable(target)\n","                data, target = data.to(device), target.to(device)\n","                data = data.view(-1, 1, input_dim).float().requires_grad_()\n","                optimizer.zero_grad()\n","                net_out = model(data)\n","            \n","                # sum up batch loss\n","                val_loss += loss_func(net_out, target).item()\n","                pred = net_out.data.max(1)[1]  # get the index of the max log-probability\n","                correct += pred.eq(target.data).sum()\n","\n","            val_loss /= len(val_loader.dataset)\n","            val_acc_array.append(correct / len(val_loader.dataset))\n","            val_loss_array.append(val_loss)\n","            print('\\nVal set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","                val_loss, \n","                correct, \n","                len(val_loader.dataset),\n","                100. * correct / len(val_loader.dataset)))\n","    \n","    return model, train_loss_array,train_acc_array,val_loss_array,val_acc_array\n"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"XM7bZ_J4WX-f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626711976220,"user_tz":-300,"elapsed":1092641,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}},"outputId":"142977dd-c637-4280-b6e1-c39e61008acf"},"source":["input_dim = 1053\n","training_epochs = 20\n","hidden_dim = 150\n","n_layers = 20\n","model = LSTM(input_dim = input_dim,hidden_dim = hidden_dim,n_layers = n_layers)\n","loss = nn.CrossEntropyLoss()\n","lr = 0.02\n","optimizer = torch.optim.SGD(model.parameters(), lr=lr)  \n","train_loader,test_loader = makeDataloaders(X,y,batch_size=32)\n","model, train_loss_array,train_acc_array,val_loss_array,val_acc_array = train(model,input_dim,train_loader, test_loader,\n","                                                                            training_epochs = training_epochs, \n","                                                                            loss_func = loss, optimizer = optimizer)\n"],"execution_count":46,"outputs":[{"output_type":"stream","text":["Train Epoch: 0 [38368/38400 (100%)]\tLoss: 0.689466, Accuracy: 19080/38400 (50%)\n","\n","\n","Val set: Average loss: 0.0217, Accuracy: 4800/9600 (50%)\n","\n","Train Epoch: 1 [38368/38400 (100%)]\tLoss: 0.691795, Accuracy: 19046/38400 (50%)\n","\n","\n","Val set: Average loss: 0.0217, Accuracy: 4800/9600 (50%)\n","\n","Train Epoch: 2 [38368/38400 (100%)]\tLoss: 0.693435, Accuracy: 19254/38400 (50%)\n","\n","\n","Val set: Average loss: 0.0217, Accuracy: 4800/9600 (50%)\n","\n","Train Epoch: 3 [38368/38400 (100%)]\tLoss: 0.693591, Accuracy: 19122/38400 (50%)\n","\n","\n","Val set: Average loss: 0.0217, Accuracy: 4800/9600 (50%)\n","\n","Train Epoch: 4 [38368/38400 (100%)]\tLoss: 0.693280, Accuracy: 19220/38400 (50%)\n","\n","\n","Val set: Average loss: 0.0217, Accuracy: 4800/9600 (50%)\n","\n","Train Epoch: 5 [38368/38400 (100%)]\tLoss: 0.693129, Accuracy: 19318/38400 (50%)\n","\n","\n","Val set: Average loss: 0.0217, Accuracy: 4800/9600 (50%)\n","\n","Train Epoch: 6 [38368/38400 (100%)]\tLoss: 0.693258, Accuracy: 19074/38400 (50%)\n","\n","\n","Val set: Average loss: 0.0217, Accuracy: 4800/9600 (50%)\n","\n","Train Epoch: 7 [38368/38400 (100%)]\tLoss: 0.692520, Accuracy: 18944/38400 (49%)\n","\n","\n","Val set: Average loss: 0.0217, Accuracy: 4800/9600 (50%)\n","\n","Train Epoch: 8 [38368/38400 (100%)]\tLoss: 0.694360, Accuracy: 18980/38400 (49%)\n","\n","\n","Val set: Average loss: 0.0217, Accuracy: 4800/9600 (50%)\n","\n","Train Epoch: 9 [38368/38400 (100%)]\tLoss: 0.691171, Accuracy: 19142/38400 (50%)\n","\n","\n","Val set: Average loss: 0.0217, Accuracy: 4800/9600 (50%)\n","\n","Train Epoch: 10 [38368/38400 (100%)]\tLoss: 0.690878, Accuracy: 19182/38400 (50%)\n","\n","\n","Val set: Average loss: 0.0217, Accuracy: 4800/9600 (50%)\n","\n","Train Epoch: 11 [38368/38400 (100%)]\tLoss: 0.690271, Accuracy: 19202/38400 (50%)\n","\n","\n","Val set: Average loss: 0.0217, Accuracy: 4800/9600 (50%)\n","\n","Train Epoch: 12 [38368/38400 (100%)]\tLoss: 0.695968, Accuracy: 19036/38400 (50%)\n","\n","\n","Val set: Average loss: 0.0217, Accuracy: 4800/9600 (50%)\n","\n","Train Epoch: 13 [38368/38400 (100%)]\tLoss: 0.696511, Accuracy: 19382/38400 (50%)\n","\n","\n","Val set: Average loss: 0.0217, Accuracy: 4800/9600 (50%)\n","\n","Train Epoch: 14 [38368/38400 (100%)]\tLoss: 0.694469, Accuracy: 19168/38400 (50%)\n","\n","\n","Val set: Average loss: 0.0217, Accuracy: 4800/9600 (50%)\n","\n","Train Epoch: 15 [38368/38400 (100%)]\tLoss: 0.694456, Accuracy: 19120/38400 (50%)\n","\n","\n","Val set: Average loss: 0.0217, Accuracy: 4800/9600 (50%)\n","\n","Train Epoch: 16 [38368/38400 (100%)]\tLoss: 0.693660, Accuracy: 19138/38400 (50%)\n","\n","\n","Val set: Average loss: 0.0217, Accuracy: 4800/9600 (50%)\n","\n","Train Epoch: 17 [38368/38400 (100%)]\tLoss: 0.694911, Accuracy: 19146/38400 (50%)\n","\n","\n","Val set: Average loss: 0.0217, Accuracy: 4800/9600 (50%)\n","\n","Train Epoch: 18 [38368/38400 (100%)]\tLoss: 0.693780, Accuracy: 19196/38400 (50%)\n","\n","\n","Val set: Average loss: 0.0217, Accuracy: 4800/9600 (50%)\n","\n","Train Epoch: 19 [38368/38400 (100%)]\tLoss: 0.695663, Accuracy: 19236/38400 (50%)\n","\n","\n","Val set: Average loss: 0.0217, Accuracy: 4800/9600 (50%)\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FTTZ1uUWJB5s","colab":{"base_uri":"https://localhost:8080/"},"outputId":"03a1842b-8d2a-4069-86a1-89126ac9aee1"},"source":["input_dim = 1053\n","training_epochs = 5\n","hidden_dim = 150\n","n_layers = 20\n","model = LSTM(input_dim = input_dim,hidden_dim = hidden_dim,n_layers = n_layers)\n","loss = nn.CrossEntropyLoss()\n","lr = 0.02\n","optimizer = torch.optim.SGD(model.parameters(), lr=lr)  \n","train_loader,test_loader = makeDataloaders(X,y,batch_size=32)\n","model, train_loss_array,train_acc_array,val_loss_array,val_acc_array = train(model,input_dim,train_loader, test_loader,\n","                                                                            training_epochs = training_epochs, \n","                                                                            loss_func = loss, optimizer = optimizer)\n","\n","\n","print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {dev_loss:.3f} | Val. Acc: {dev_acc*100:.2f}% |')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0% [██████████████████████████████] 100% | ETA: 00:00:00\n","Total time elapsed: 00:00:06\n","0% [██████████████████████████████] 100% | ETA: 00:00:00"],"name":"stderr"},{"output_type":"stream","text":["| Epoch: 01 | Train Loss: 0.691 | Train Acc: 52.55% | Val. Loss: 0.687 | Val. Acc: 56.56% |\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Total time elapsed: 00:00:00\n","0% [██████████████████████████████] 100% | ETA: 00:00:00\n","Total time elapsed: 00:00:06\n","0% [██████████████████████████████] 100% | ETA: 00:00:00"],"name":"stderr"},{"output_type":"stream","text":["| Epoch: 02 | Train Loss: 0.656 | Train Acc: 61.22% | Val. Loss: 0.622 | Val. Acc: 64.79% |\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Total time elapsed: 00:00:00\n","0% [██████████████████████████████] 100% | ETA: 00:00:00\n","Total time elapsed: 00:00:06\n","0% [██████████████████████████████] 100% | ETA: 00:00:00"],"name":"stderr"},{"output_type":"stream","text":["| Epoch: 03 | Train Loss: 0.634 | Train Acc: 64.09% | Val. Loss: 0.613 | Val. Acc: 66.71% |\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Total time elapsed: 00:00:00\n","0% [██████████████████████████████] 100% | ETA: 00:00:00\n","Total time elapsed: 00:00:06\n","0% [██████████████████████████████] 100% | ETA: 00:00:00"],"name":"stderr"},{"output_type":"stream","text":["| Epoch: 04 | Train Loss: 0.626 | Train Acc: 65.03% | Val. Loss: 0.600 | Val. Acc: 67.38% |\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Total time elapsed: 00:00:00\n","0% [██████████████████████████████] 100% | ETA: 00:00:00\n","Total time elapsed: 00:00:06\n","0% [██████████████████████████████] 100% | ETA: 00:00:00"],"name":"stderr"},{"output_type":"stream","text":["| Epoch: 05 | Train Loss: 0.616 | Train Acc: 65.61% | Val. Loss: 0.595 | Val. Acc: 68.09% |\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Total time elapsed: 00:00:00\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"5aWVBo_35aKX"},"source":["##GRU"]},{"cell_type":"markdown","metadata":{"id":"Ub4lzq_2WX-g"},"source":["### Word Cloud"]},{"cell_type":"code","metadata":{"id":"WJ0QcbT4WX-h","colab":{"base_uri":"https://localhost:8080/","height":135},"executionInfo":{"status":"error","timestamp":1626709259184,"user_tz":-300,"elapsed":423,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}},"outputId":"af35d846-0231-42b1-989a-9d4440aa0bd4"},"source":["# Word Cloud that shows the positive and negative words and thier frequence \n","# make a dictionary of words with their word_count\n","counts = tokenizer.word_counts\n","# use you trained model to prodict sentiment of these words ??\n","# Saperate positive sentiment words and negative sentiment words with their counts ??\n","# make word cloud according\n","plt.figure(figsize=(??,??))\n","wordcloud = WordCloud(background_color=??,max_words=??).generate_from_frequencies(??)\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.show()"],"execution_count":32,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-32-b22d8cb202a9>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    plt.figure(figsize=(??,??))\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","metadata":{"id":"gQTFe2vLWX-h","executionInfo":{"status":"aborted","timestamp":1626709134025,"user_tz":-300,"elapsed":570,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}}},"source":["# Word Cloud that shows main words used in reviews that are positive or negative  \n","# Randomly select one sentence that has positive sentiment and one with negative sentiment \n","??\n","# make word cloud of those sentences and see what are main words used in positive review and in negative reviews\n","# Similarly you can make word clouds of all positive reviews and negative reviews to see the most repeating words in positive sentiments and negative sentiments\n","plt.figure(figsize=(??,??))\n","plt.title('Prediceted Sentiment on this review: '+??)\n","wordcloud = WordCloud(background_color='White',max_words=100).generate(??)\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UOxTvr53XPMu","executionInfo":{"status":"aborted","timestamp":1626708178898,"user_tz":-300,"elapsed":16,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzx9hpa1YW6ppoIAXPYSl9sAC-ejJJ_91d6ybxgQ=s64","userId":"14177622881280380240"}}},"source":[""],"execution_count":null,"outputs":[]}]}