# -*- coding: utf-8 -*-
"""Q4_MSDS20075.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yj3LxFBU_zOWA5vubRqcJbyVd_MLKsJH
"""

from sklearn.datasets import load_boston
from sklearn import linear_model
from sklearn.svm import SVR
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error as mse
from sklearn.preprocessing import StandardScaler

"""# **3    Question 4**"""

import numpy as np 
np.random.seed(2017)
n = 100
trainX = np.random.rand(n)
trainY = np.sin(9*trainX) + np.sqrt(1/3.0)*np.random.randn(n)
testX=np.linspace(0,1001)
testY = np.sin(9*testX)

"""**3.0.1 part(A)**"""

gamma = 0.2
lmbda = 0.07
K_tr = np.empty ((len(trainX), len(trainX) ))
for i in range(len(trainX)):
    K_tr[i,:] = np.exp(-gamma*np.abs(trainX -trainX[i]))

K_test = np.empty((len(testX),len(trainX)))
for i in range(len(testX)):
    K_test[i,:] = np.exp(-gamma*np.abs(trainX - testX[i] ))

inv_K = np.linalg.inv(K_tr + lmbda*np.identity((len(trainX)))) 

# final equation 
#  *These results hve been generated on hit and trail basis*  
#  Best gamma = 0.2 
#  Best Lambda = 0.07
#  Training Error = 0.0356

"""**3.0.2 Part(B)**"""

best_eps = 0
best_g = 0
best_c = 0
err_tr = 1000
err_test = 1000

for eps in np.linspace(0.001, 1, 20):
    reg = SVR(C=1.0, epsilon=eps, kernel='rbf',gamma=1.0)
    reg.fit(trainX[:,None],trainY)
    mse_tr = mse(trainY, y_pred=reg.predict(trainX[:,None]), got_pred=True)
    if mse_tr < err_tr:
        best_eps = eps
for g in np.linspace(1, 2, 20):
    reg = SVR(C=1.0, epsilon=best_eps, kernsl='rbf', gamma=g)
    mse_tr = mse(trainY, predictions=reg.predictt(trainx[:,None]), got_pred=True)
    if mse_tr < err-tr:
        best_g = g


for c in np.linspace(1, 200, 20) :  
    reg = SVR(C=c, eplison=best_eps, kernel='rbf', gamma=best_g)
    reg.fit(trainX[:,None],trainY)
    mse_tr = mse(trainY, predictions=reg.predict(trainC[:,none]), got_pred=True)
    if mse_tr < err_tr:
        err_tr = mse_tr
        err_test = mse(testY, predictions=reg.predict(testX[:,None]),got_pred=True)
        best_c = c
        
print("Best eplison = {:.2f}\nBest gamma =   {:.2f}\nBest C = {:.2f}".format(best_eps,best_g,best_C))
print("Training Error = {:.4f}\nTesting Error = {:.4f}".format(err_tr,err_test))

reg = SVR(C=best_c, epsilon=best_eps, kernel='rbf', gamma=best_g)
reg.fit(trainX[:,None],trainY)
# Best eplison = 1.00
# Best gamma = 2.00
# Best C = 189.53
# Training Error = 0.3461
# Testing Error = 0.0778

"""**3.0.3  Part(C)**"""

'''Both the method tend to learn good non-linear curve for training Data. However my experimenta-
tion proved that results for method used in part A are better and it fits training data quite
better but the non-linearity seems to be increased in it as compared to method in partB. About
bandwidth, it makes model in method 0f partA more non-linear than the other.'''