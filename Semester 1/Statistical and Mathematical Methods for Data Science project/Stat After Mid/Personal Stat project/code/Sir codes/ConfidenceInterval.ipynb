{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"colab":{"name":"ConfidenceInterval.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"OW0t6VxaBaDl"},"source":["# This lab is designed by Dr. Faisal Bukhari, PUCIT, PU, Lahore, Pakistan"]},{"cell_type":"markdown","metadata":{"id":"oZx-PzfRBaEO"},"source":["## Numpy\n","## Numpy is the core library for scientific computing in Python. \n","## It provides a high-performance multidimensional array object, and tools for working with these arrays. "]},{"cell_type":"markdown","metadata":{"id":"1i8ehtwaBaES"},"source":["## SciPy is an Open Source Python-based library, which is used in mathematics, scientific computing, Engineering, and technical computing. SciPy also pronounced as \"Sigh Pi.\""]},{"cell_type":"markdown","metadata":{"id":"WqIngftFBaEU"},"source":["# Numpy VS SciPy\n","Numpy:\n","\n","Numpy is written in C and use for mathematical or numeric calculation.\n","It is faster than other Python Libraries\n","Numpy is the most useful library for Data Science to perform basic calculations.\n","Numpy contains nothing but array data type which performs the most basic operation like sorting, shaping, indexing, etc.\n","\n","SciPy:\n","SciPy is built in top of the NumPy\n","SciPy is a fully-featured version of Linear Algebra while Numpy contains only a few features.\n","Most new Data Science features are available in Scipy rather than Numpy.\n","\n","Reference: https://www.guru99.com/scipy-tutorial.html"]},{"cell_type":"markdown","metadata":{"id":"KDMjJZTIBaEh"},"source":["scipy.stats.sem\n","scipy.stats.sem(a, axis=0, ddof=1)[source]\n","Calculates the standard error of the mean (or standard error of measurement) of the values in the input array.\n","\n","Parameters:\t\n","a : array_like\n","\n","An array containing the values for which the standard error is returned.\n","\n","axis : int or None, optional.\n","\n","If axis is None, ravel a first. If axis is an integer, this will be the axis over which to operate. Defaults to 0.\n","\n","ddof : int, optional\n","\n","Delta degrees-of-freedom. How many degrees of freedom to adjust for bias in limited samples relative to the population estimate of variance. Defaults to 1.\n","\n","Returns:\t\n","s : ndarray or float\n","\n","The standard error of the mean in the sample(s), along the input axis.\n","\n","Notes\n","\n","The default value for ddof is different to the default (0) used by other ddof containing routines, such as np.std nd stats.nanstd.\n","### Reference: https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.sem.html"]},{"cell_type":"markdown","metadata":{"id":"vGp8a7whBaEk"},"source":["# Example: The contents of seven similar containers of sulfuric acid are 9.8, 10.2, 10.4, 9.8, 10.0, 10.2, and 9.6 liters. Find a 95% confidence interval for the mean contents of all such containers, assuming an approximately normal distribution.\n"]},{"cell_type":"code","metadata":{"id":"QjWPL7FkBaEw","outputId":"7188072e-ec1c-40e9-9954-6b9bacfa95b3"},"source":["from scipy import stats\n","import numpy as np\n","\n","arr = [9.8, 10.2, 10.4, 9.8, 10.0, 10.2, 9.6]  # data\n","alpha = 0.05                # significance level = 5%\n","df = len(arr) - 1           # degress of freedom = 6\n","t = stats.t.ppf(1 - alpha/2, df) # t-critical value for 95% CI = 2.447\n","print(round(t,3))\n","\n","s = np.std(arr, ddof=1) # sample standard deviation = 0.2828\n","print(round(s,4))\n","n = len(arr)\n","\n","lower = np.mean(arr) - (t * s / np.sqrt(n))\n","print(round(lower,4))\n","upper = np.mean(arr) + (t * s / np.sqrt(n))\n","print(round(upper,4))\n","(lower, upper)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2.447\n","0.2828\n","9.7384\n","10.2616\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(9.738414120176683, 10.261585879823317)"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"yV_ZESroBaFE","outputId":"f02a2462-d97b-47fe-bdda-d932ced06e5a"},"source":["#stats.t.interval(1 - alpha, len(arr) - 1, loc=np.mean(arr), scale=stats.sem(arr))\n","\n","from scipy import stats\n","import numpy as np\n","alpha = 0.05                       # significance level = 5%\n","arr = [9.8, 10.2, 10.4, 9.8, 10.0, 10.2, 9.6]\n","df = len(arr) - 1                  # degress of freedom = 6\n","\n","stats.t.interval(1 - alpha, len(arr) - 1, loc=np.mean(arr), scale=stats.sem(arr))\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(9.738414120176683, 10.261585879823317)"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"ZWLw4hh-BaFQ"},"source":["# Exercise 9.40: In a study conducted at Virginia Tech on the development of ectomycorrhizal, a symbiotic relationship between the roots of trees and a fungus, in which minerals are transferred from the fungus to the trees and sugars from the trees to the fungus, 20 northern red oak seedlings exposed to the fungus Pisolithus tinctorus were grown in a greenhouse. All seedlings were planted in the same type of soil and received the same amount of sunshine and water. Half received no nitrogen at planting time, to serve as a control, and the other half received 368 ppm of nitrogen in the form NaNO3. The stem weights, in grams, at the end of 140 days were recorded as follows:\n","No Nitrogen     Nitrogen\n","0.32            0.26\n","0.53            0.43\n","0.28            0.47\n","0.37            0.49\n","0.47            0.52\n","0.43            0.75\n","0.36            0.79\n","0.42            0.86\n","0.38            0.62\n","0.43            0.46\n","# Construct a 95% confidence interval for the difference in the mean stem weight between seedlings that receive no nitrogen and those that receive 368 ppm of nitrogen. Assume the populations to be normally distributed with equal variances."]},{"cell_type":"code","metadata":{"id":"JHfLMtxdBaFT","outputId":"d560b742-e1a5-46a4-8d8e-27f47d46376f"},"source":["x1 = [0.32, 0.53, 0.28, 0.37, 0.47, 0.43, 0.36, 0.42, 0.38, 0.43]\n","x2 = [0.26, 0.43, 0.47, 0.49, 0.52, 0.75, 0.79, 0.86, 0.62, 0.46]\n","print(np.mean(x1))\n","print(np.mean(x2))\n","\n","alpha = 0.05                                                     # significance level = 5%\n","n1, n2 = len(x1), len(x2)                                        # sample sizes\n","var1, var2 = np.var(x1, ddof = 1), np.var(x2, ddof = 1)          # sample variances\n","sp = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2)) # pooled standard deviation\n","print(\"sp =\", sp)\n","df = n1 + n2 - 2                                                 # degrees of freedom\n","t = stats.t.ppf(1 - alpha/2, df)                                 # t-critical value for 95% CI\n","print(\"tabulated\", round(t,4))\n","t_cal = (np.mean(x1) - np.mean(x2))/(sp * np.sqrt(1 / len(x1) + 1 / len(x2)))\n","print(\"t calulated:\", round(t_cal,4))\n","\n","lower = (np.mean(x1) - np.mean(x2)) - t * sp * np.sqrt(1 / len(x1) + 1 / len(x2)) \n","upper = (np.mean(x1) - np.mean(x2)) + t  * sp * np.sqrt(1 / len(x1) + 1 / len(x2)) \n","(lower, upper)\n","\n","lower = (np.mean(x2) - np.mean(x1)) - t * sp * np.sqrt(1 / len(x1) + 1 / len(x2)) \n","upper = (np.mean(x2) - np.mean(x1)) + t * sp * np.sqrt(1 / len(x1) + 1 / len(x2)) \n","(lower, upper)\n","\n","stats.ttest_ind(x1, x2, equal_var=False)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.39899999999999997\n","0.5650000000000001\n","sp = 0.14172351800444255\n","tabulated 2.1009\n","t calulated: -2.6191\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["Ttest_indResult(statistic=-2.6190944840455472, pvalue=0.022863946155002354)"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"Mjcax9AMBaFf","outputId":"0681e980-7014-43c7-d38e-1114907e7950"},"source":["\n","\n","alpha = 0.10                                                     # significance level = 5%\n","n1, n2 = 12, 10                                                  # sample sizes\n","s1, s2 = 0.771*0.771, 0.448*0.448             # sample variances\n","print(s1)\n","print(s2)\n","sp = np.sqrt(((n1 - 1) * s1 + (n2 - 1) * s2) / (n1 + n2 - 2))     # pooled standard deviation\n","print(s)\n","df = n1 + n2 - 2                                                 # degrees of freedom\n","print(df)\n","t = stats.t.ppf(1 - alpha/2, df)                                 # t-critical value for 95% CI\n","print(t)\n","\n","lower = (3.11 - 2.04) - t * sp * np.sqrt(1 / n1 + 1 / n2 )\n","upper = (3.11 - 2.04) + t * sp * np.sqrt(1 / n1 + 1 / n2 )\n","round(lower,4), round(upper,4)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.594441\n","0.20070400000000002\n","0.28284271247461884\n","20\n","1.7247182429207857\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(0.593, 1.547)"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"q9xSM9qiBaFj"},"source":["# 9.46 The following data represent the running times of films produced by two motion-picture companies.\n","Company Time (minutes)\n","I 103 94 110 87 98\n","II 97 82 123 92 175 88 118\n","Compute a 90% confidence interval for the difference between the average running times of films produced by\n","the two companies. Assume that the running-time differences are approximately normally distributed with unequal variances."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QYy0UOyVBaFl","outputId":"8cccafc2-f1b3-459c-bf7e-e8f713a39555"},"source":["from scipy import stats\n","import numpy as np\n","x1 = [103, 94, 110, 87, 98]\n","x2 = [97, 82, 123, 92, 175, 88, 118]\n","alpha = 0.10                                                       # significance level = 5%\n","n1 = len(x1)                                                       # sample size\n","n2 = len(x2)                                                       # sample size\n","var1 = np.var(x1, ddof=1)                                          # sample variance\n","var2 = np.var(x2, ddof=1)  \n","# sample variance\n","print(\"Variance:\", round(var1,4))\n","print(\"Variance:\", round(var2,4))\n","print(\"Sd:\", round(np.sqrt(var1),4))\n","print(\"Sd:\", round(np.sqrt(var2),4))\n","s = np.sqrt(var1/n1 + var2/n2)                                     # combined standard deviation\n","print(round(s,4))\n","\n","df = (var1/n1 + var2/n2)**2 / ((var1/n1)**2/(n1-1) + (var2/n2)**2/(n2-1))  # degrees of freedom\n","print(\"df=\",round(df, 0))\n","\n","t = stats.t.ppf(1 - alpha/2, df)                                   # t-critical value for 95% CI\n","print(\"t tabulated:\", round(t,4))\n","\n","print(\"mean 1 = \", np.mean(x1))\n","print(\"mean 2 = \",np.mean(x2))\n","\n","lower = (np.mean(x2) - np.mean(x1)) - t  * s\n","upper = (np.mean(x2) - np.mean(x1)) + t  * s\n","print(round(lower,4), round(upper,4))\n","lower = (np.mean(x1) - np.mean(x2)) - t  * s\n","upper = (np.mean(x1) - np.mean(x2)) + t  * s\n","print(round(lower,4), round(upper,4))\n","mu1 = 0\n","mu2 = 0\n","tcal = ((np.mean(x2) - np.mean(x1)) - (mu1 - mu2))/np.sqrt(var1/n1 + var2/n2)\n","\n","print(\"t calulated:\", round(tcal,4))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Variance: 76.3\n","Variance: 1035.9048\n","Sd: 8.735\n","Sd: 32.1855\n","12.7768\n","df= 7.0\n","t tabulated: 1.8872\n","mean 1 =  98.4\n","mean 2 =  110.71428571428571\n","-11.7981 36.4267\n","-36.4267 11.7981\n","t calulated: 0.9638\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JefJ740DBaFv"},"source":["# 9.46 The following data represent the running times of films produced by two motion-picture companies.\n","Company Time (minutes)\n","I 103 94 110 87 98\n","II 97 82 123 92 175 88 118\n","Compute a 90% confidence interval for the difference between the average running times of films produced by\n","the two companies. Assume that the running-time differences are approximately normally distributed with equal variances."]},{"cell_type":"code","metadata":{"id":"euMQGDX3BaFy","outputId":"1c5494c2-d7e5-4584-e635-1c349db44106"},"source":["from scipy import stats\n","import numpy as np\n","x1 = [103, 94, 110, 87, 98]\n","x2 = [97, 82, 123, 92, 175, 88, 118]\n","\n","print(np.mean(x1))\n","print(np.mean(x2))\n","\n","alpha = 0.10                                                     # significance level = 5%\n","n1, n2 = len(x1), len(x2)                                        # sample sizes\n","var1, var2 = np.var(x1, ddof = 1), np.var(x2, ddof = 1)          # sample variances\n","sp = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2)) # pooled standard deviation\n","print(\"sp = \", round(sp,4))\n","df = n1 + n2 - 2                                                 # degrees of freedom\n","print(\"df = \",df)\n","t = stats.t.ppf(1 - alpha/2, df)                                 # t-critical value for 95% CI\n","print(\"t-tabulated = \",round(t,4))\n","t_cal = (np.mean(x2) - np.mean(x1))/(sp * np.sqrt(1 / len(x1) + 1 / len(x2)))\n","print(\"t-cal = \",round(t_cal,4))\n","lower = (np.mean(x1) - np.mean(x2)) - t * sp * np.sqrt(1 / len(x1) + 1 / len(x2)) \n","upper = (np.mean(x1) - np.mean(x2)) + t  * sp * np.sqrt(1 / len(x1) + 1 / len(x2)) \n","(lower, upper)\n","\n","lower = (np.mean(x2) - np.mean(x1)) - t * sp * np.sqrt(1 / len(x1) + 1 / len(x2)) \n","upper = (np.mean(x2) - np.mean(x1)) + t * sp * np.sqrt(1 / len(x1) + 1 / len(x2)) \n","(round(lower,4), round(upper,4))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["98.4\n","110.71428571428571\n","sp =  25.5355\n","df =  10\n","t-tabulated =  1.8125\n","t-cal =  0.8236\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(-14.7858, 39.4143)"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"3L6PIL-UBaGE","outputId":"92d80fe9-650d-4601-c4e2-dcca18ca6ade"},"source":["from scipy import stats\n","import numpy as np\n","a = np.arange(15).reshape(5,3)\n","print(a)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[ 0  1  2]\n"," [ 3  4  5]\n"," [ 6  7  8]\n"," [ 9 10 11]\n"," [12 13 14]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wjjaFxSbBaGG","outputId":"d36dad26-cd8f-43ff-c067-5b73f65a5c60"},"source":["stats.sem(a, axis=None, ddof=1)\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.10690449676496969"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"66JsROISBaGH","outputId":"b862d210-38cc-458e-8c59-c6f04cc600d4"},"source":["# 9.8, 10.2, 10.4, 9.8, 10.0, 10.2, and 9.6 \n","a=[9.8, 10.2, 10.4, 9.8, 10.0, 10.2, 9.6]\n","stats.sem(a, axis=None, ddof=6)\n","\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.2618614682831907"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"QF3HXAHYBaGI","outputId":"5fc6cc69-9638-4e12-9bab-18bb8f7e82a9"},"source":["import numpy as np\n","a=[9.8, 10.2, 10.4, 9.8, 10.0, 10.2, 9.6]\n","# np.std(a, axis=None, dtype=None, out=None, ddof=1)\n","std_err = np.std(a,ddof=1)\n","print(round(std_err,4))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.2828\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"i7TkckxbBaGK","outputId":"682d9c84-cf06-42cf-dfae-85bd4562f75b"},"source":["from scipy.stats import sem, t\n","from scipy import mean\n","confidence = 0.95\n","data = [9.8, 10.2, 10.4, 9.8, 10.0, 10.2, 9.6]\n","std_err = sem(data)\n","print(std_err)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.10690449676496969\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jJcjGL5lBaGL","outputId":"4f9777a1-39aa-4bf7-f303-1426a0406346"},"source":["from scipy.stats import sem, t\n","from scipy import mean\n","\n","confidence = 0.95\n","data = [1, 2, 3, 4, 5]\n","n = len(data)\n","m = mean(data)\n","std_err = sem(data)\n","\n","h = std_err * t.ppf((1 + confidence) / 2, n - 1)\n","start = m - h\n","\n","print(start)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1.036756838522439\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zdOosHTyBaGM","outputId":"e466631e-97a5-4d0c-b5c0-ccab7a4cd616"},"source":["end = m + h\n","print (end)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["4.9632431614775605\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"49xLrcfDBaGO"},"source":["# Example 6.11: The average zinc concentration recovered from a sample of measurements taken in 36 different locations in a river is found to be 2.6 grams per milliliter. Find the 95% and 99% confidence intervals for the mean zinc concentration in the river. Assume that the population standard deviation is 0.3 gram per milliliter."]},{"cell_type":"code","metadata":{"id":"_sQNfHidBaGP","outputId":"388ecdfc-2109-4e5c-a598-bca31c14ffcc"},"source":["from scipy.stats import norm\n","import numpy as np\n","ztab = round(norm.ppf(0.975), 2)  #95% confidence interal i.e. 1 - alplh/2 = 0.975\n","print(ztab)\n","#ztab = round(norm.ppf(0.995), 4)  #99% confidence interal i.e. 1 - alplh/2 = 0.995\n","#print(ztab)\n","\n","sigma = 0.3                       \n","n = 36\n","meanx = 2.6\n","print(ztab * sigma / np.sqrt(n))\n","lower = meanx  - ztab * sigma / np.sqrt(n)\n","print(round(lower,4))\n","upper = meanx  + ztab * sigma / np.sqrt(n)\n","print(round(upper,4))\n","(lower, upper)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1.96\n","0.09799999999999999\n","2.502\n","2.698\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(2.5020000000000002, 2.698)"]},"metadata":{"tags":[]},"execution_count":123}]},{"cell_type":"code","metadata":{"id":"MpCTsU19BaGR","outputId":"015f1829-0721-4c26-f032-c6526a669017"},"source":["from scipy.stats import norm\n","import numpy as np\n","\n","ztab = round(norm.ppf(0.995), 4)  #99% confidence interal i.e. 1 - alplh/2 = 0.995\n","print(ztab)\n","\n","sigma = 0.30                       \n","n = 36\n","meanx = 2.6\n","lower = meanx  - (ztab * sigma / np.sqrt(n))\n","print(round(lower,4))\n","upper = meanx  + (ztab * sigma / np.sqrt(n))\n","print(round(upper,4))\n","(lower, upper)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2.5758\n","2.4712\n","2.7288\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(2.47121, 2.72879)"]},"metadata":{"tags":[]},"execution_count":119}]},{"cell_type":"code","metadata":{"id":"APUmlWkzBaGT","outputId":"0eb54488-abc4-418e-a2bc-9df3dc27934e"},"source":["from scipy.stats import norm\n","import numpy as np\n","ztab = round(norm.ppf(0.975), 4)  #95% confidence interal\n","print(ztab)\n","ztab = round(norm.ppf(0.995), 4)  #99% confidence interal\n","print(ztab)\n","ztab = round(norm.ppf(0.95), 4)  #90% confidence interal\n","print(ztab)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1.96\n","2.5758\n","1.6449\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0NKbWSkCBaGU","outputId":"b0caeaf2-a1c7-4046-950f-2f0313d9ed67"},"source":["from scipy.stats import norm\n","import numpy as np\n","ztab = round(norm.cdf(0.6250), 4)  #90% confidence interal\n","print(ztab)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.734\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DpPuIEixBaGW","outputId":"d53f5492-3283-4d42-ae2d-5cc90c94f55c"},"source":["from scipy.stats import norm\n","import numpy as np\n","alpha = 0.375\n","ztab = round(norm.ppf(1 - alpha/), 4)  #90% confidence interal\n","print(ztab)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["-0.8871\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ms3hpIKoBaGY","outputId":"f01a0b6f-1329-4ff2-e677-73c53389b968"},"source":["import scipy.stats as stats\n","zvalue = stats.norm.ppf(0.8) #0.8 = area to left\n","\n","#print corresponding zvalue\n","print(round(zvalue, 4))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.8416\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ebiIzMpnBaGb","outputId":"ba937648-62d6-4971-b2b3-7bf671150ed8"},"source":["import scipy.stats as stats\n","mean = 40\n","sd = 6\n","z = stats.norm.ppf(0.8) #0.8 = area to left\n","x = sd *z + mean\n","print(round(x,4))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["45.0497\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IHbfpNvKBaGp","outputId":"886135d8-810d-4a96-943a-d0686fc3bd59"},"source":["import scipy.stats as stats\n","mean = 40\n","sd = 6\n","z = stats.norm.ppf(0.20)                                           #0.8 = area to left\n","p20 = sd *z + mean\n","print(round(p20, 4))\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["34.9503\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"co3wvCeYBaGr","outputId":"ac5bcade-a228-42b7-cfcd-cb2d68e4e13a"},"source":["from scipy import stats\n","df = 9\n","alpha = .05\n","t = stats.t.ppf(alpha, df) \n","print(round(t,4))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["-1.8331\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1NQ-9s-2BaGs","outputId":"20bf3d79-131a-4d1a-c5cf-3a0af14eab51"},"source":["from scipy.stats import norm\n","import numpy as np\n","alpha = 0.375\n","ztab = 1-round(norm.ppf(0.63), 4)  #90% confidence interal\n","print(ztab)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.6681\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"l_g6w3x5BaGt"},"source":["import scipy.stats\n","scipy.stats.norm(loc=100, scale=12)\n","#where loc is the mean and scale is the std dev\n","#if you wish to pull out a random number from your distribution\n","scipy.stats.norm.rvs(loc=100, scale=12)\n","\n","#To find the probability that the variable has a value LESS than or equal\n","#let's say 113, you'd use CDF cumulative Density Function\n","scipy.stats.norm.cdf(113,100,12)\n","Output: 0.86066975255037792\n","#or 86.07% probability\n","\n","#To find the probability that the variable has a value GREATER than or\n","#equal to let's say 125, you'd use SF Survival Function \n","scipy.stats.norm.sf(125,100,12)\n","Output: 0.018610425189886332\n","#or 1.86%\n","\n","#To find the variate for which the probability is given, let's say the \n","#value which needed to provide a 98% probability, you'd use the \n","#PPF Percent Point Function\n","scipy.stats.norm.ppf(.98,100,12)\n","Output: 124.64498692758187"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aggnuzyJBaGu","outputId":"c2e4395e-1976-40a1-dbe0-2f8cd7802926"},"source":["import scipy.stats as stats\n","prob = round(stats.norm.cdf(45,40,8),4)   # x, mean, sd\n","print(prob)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.734\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"q7inuc2wBaGw"},"source":[""],"execution_count":null,"outputs":[]}]}