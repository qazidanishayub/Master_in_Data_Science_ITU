{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"TF_Sample_Code.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"9aV8LDjJosXH"},"source":["### TF-Weightage Sample Code\n","This is a sample code to give students an idea of how Term-Frequency weightage model can be applied to calculate relevance score of documents.\n","\n","The example is taken from **lecture 3.2**."]},{"cell_type":"markdown","metadata":{"id":"pELOOZAyosXI"},"source":["<h3 style = 'color:purple;'>Vector Space Model (TF-Weightage Model)</h3>\n","\n","$$ f(q,d) = sim(q,d) =  \\sum_{i=1}^n x_iy_i $$ \n","q = (x_1,.....,x_n) <br>\n","d = (y_1,.....,y_n) <br>\n","x_i = count of word W_i in query. <br>\n","y_i = count of word W_i in doc."]},{"cell_type":"code","metadata":{"id":"HAHZZUwzosXI"},"source":["#lets say we have the following documents\n","documents = {\n","    \"d1\" : \"news about\",\n","    \"d2\" : \"news about organic food campaign\",\n","    \"d3\" : \"news of presidential campaign\",\n","    \"d4\" : \"news of presidential campaign presidential candidate\",\n","    \"d5\" : \"news of organic food campaign campaign campaign campaign\"\n","} # a dictionary with doc# as key and doc content as value"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IckjbMJeosXN","outputId":"313513ea-1ee6-43c6-9427-079806df434b"},"source":["#visualize the dictionary\n","documents"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'d1': 'news about',\n"," 'd2': 'news about organic food campaign',\n"," 'd3': 'news of presidential campaign',\n"," 'd4': 'news of presidential campaign presidential candidate',\n"," 'd5': 'news of organic food campaign campaign campaign campaign'}"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"j7jQpDSSosXT"},"source":["#create a corpus ccontaining the vocabulary of words in the documents\n","corpus = [] # a list that will store words of the vocabulary\n","for doc in documents.values(): #iterate through documents \n","    for word in doc.split(): #go through each word in the current doc\n","        if not word in corpus: \n","            corpus.append(word) #add word in corpus if not already added"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XEfkyp_PosXV","outputId":"4a214ead-3dd0-4728-9d30-a931be9a23e8"},"source":["#visualize the corpus \n","corpus"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['news',\n"," 'about',\n"," 'organic',\n"," 'food',\n"," 'campaign',\n"," 'of',\n"," 'presidential',\n"," 'candidate']"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"Yn9mp19XosXY"},"source":["#lets create a dictionary within a dictionary to store term-frequncy for each doc\n","tf_docs = {} #empty dictionary\n","for doc_id in documents.keys(): #iterate through doc# (d1,d2,...,d5)\n","    tf_docs[doc_id] = {} #create empty dictionary for each doc# key"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ra8W3QAWosXa","outputId":"2fe0a53d-3def-4aaa-917e-ac35824924e1"},"source":["#visualize the state of tf_docs\n","tf_docs"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'d1': {}, 'd2': {}, 'd3': {}, 'd4': {}, 'd5': {}}"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"3VaHOteVosXd"},"source":["As you can see, we have created a dictionary against each doc, now we have to use the created dictionaries to store term frequencies for each doc."]},{"cell_type":"code","metadata":{"id":"xScwmMFyosXd"},"source":["#lets start on storing term-frequencies for every doc\n","for word in corpus: #iterate through words in the corpus\n","    for doc_id,doc in documents.items(): #iterate through documents dictionary\n","        tf_docs[doc_id][word] = doc.count(word) #store term-frequency for the word in each doc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kNkvum9JosXg","outputId":"cde99c53-ac39-48f3-814d-f8060cf63b12"},"source":["tf_docs #visualize calculated term frequencies"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'d1': {'news': 1,\n","  'about': 1,\n","  'organic': 0,\n","  'food': 0,\n","  'campaign': 0,\n","  'of': 0,\n","  'presidential': 0,\n","  'candidate': 0},\n"," 'd2': {'news': 1,\n","  'about': 1,\n","  'organic': 1,\n","  'food': 1,\n","  'campaign': 1,\n","  'of': 0,\n","  'presidential': 0,\n","  'candidate': 0},\n"," 'd3': {'news': 1,\n","  'about': 0,\n","  'organic': 0,\n","  'food': 0,\n","  'campaign': 1,\n","  'of': 1,\n","  'presidential': 1,\n","  'candidate': 0},\n"," 'd4': {'news': 1,\n","  'about': 0,\n","  'organic': 0,\n","  'food': 0,\n","  'campaign': 1,\n","  'of': 1,\n","  'presidential': 2,\n","  'candidate': 1},\n"," 'd5': {'news': 1,\n","  'about': 0,\n","  'organic': 1,\n","  'food': 1,\n","  'campaign': 4,\n","  'of': 1,\n","  'presidential': 0,\n","  'candidate': 0}}"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"txUC1rzWosXi","executionInfo":{"status":"error","timestamp":1605224907829,"user_tz":-300,"elapsed":1371,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi5xy9eDxsjOUQqFg5ZTnVkZn4Jsy24Nij3bgNZ=s64","userId":"14177622881280380240"}},"outputId":"58eb85d0-134d-4eb4-a5f4-6e31ecb2a3bc","colab":{"base_uri":"https://localhost:8080/","height":163}},"source":["tf_docs['d3'] #checking term frequencies for d3"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-8f58b223ce13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'd3'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#checking term frequencies for d3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'tf_docs' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"wPSsr4sfosXk"},"source":["### Querying the documents for relevance scores\n","Since we have calculated the term frequencies for all the documents in our collection, let us calcualte the relevance score of each document for a given query."]},{"cell_type":"code","metadata":{"id":"sU70PLYrosXl","executionInfo":{"status":"ok","timestamp":1605225325639,"user_tz":-300,"elapsed":1298,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi5xy9eDxsjOUQqFg5ZTnVkZn4Jsy24Nij3bgNZ=s64","userId":"14177622881280380240"}},"outputId":"56809bbc-a4fb-4fe1-c06d-6c4c0bb31414","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["query = \"news about presidential campaign\" #the query\n","query"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'news about presidential campaign'"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"5374Vh18osXo","executionInfo":{"status":"ok","timestamp":1605225326884,"user_tz":-300,"elapsed":974,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi5xy9eDxsjOUQqFg5ZTnVkZn4Jsy24Nij3bgNZ=s64","userId":"14177622881280380240"}}},"source":["query_vocab = [] # will store the unique words that occur in the query\n","for word in query.split():\n","    if word not in query_vocab:\n","        query_vocab.append(word)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"CdHBFRm7osXq","executionInfo":{"status":"ok","timestamp":1605225328182,"user_tz":-300,"elapsed":1144,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi5xy9eDxsjOUQqFg5ZTnVkZn4Jsy24Nij3bgNZ=s64","userId":"14177622881280380240"}},"outputId":"53f77ef6-f1b8-42e1-c6d8-d7119dfe924f","colab":{"base_uri":"https://localhost:8080/"}},"source":["query_vocab # the unique words in the query"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['news', 'about', 'presidential', 'campaign']"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"WyIIztsYosXt","executionInfo":{"status":"ok","timestamp":1605225339858,"user_tz":-300,"elapsed":1108,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi5xy9eDxsjOUQqFg5ZTnVkZn4Jsy24Nij3bgNZ=s64","userId":"14177622881280380240"}}},"source":["query_wc = {} # a dictionary to store count of a word in the query (i.e x_i according to lecture slides terminology)\n","for word in query_vocab:\n","    query_wc[word] = query.split().count(word)"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"h5zFgvXlosXv","executionInfo":{"status":"ok","timestamp":1605225343327,"user_tz":-300,"elapsed":1126,"user":{"displayName":"Qazi Danish Ayub","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi5xy9eDxsjOUQqFg5ZTnVkZn4Jsy24Nij3bgNZ=s64","userId":"14177622881280380240"}},"outputId":"413ee763-fd01-46c0-b7c4-e060d6ec944a","colab":{"base_uri":"https://localhost:8080/"}},"source":["query_wc # the count of each word that occurs in the query"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'about': 1, 'campaign': 1, 'news': 1, 'presidential': 1}"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"tHpRzcxSosXx"},"source":["relevance_scores = {} # a dictionary that will store the relevance score for each doc\n","# doc_id will be the key and relevance score the value for this dictionary\n","for doc_id in documents.keys():\n","    score = 0 #initialze the score for the doc to 0 at the start\n","    for word in query_vocab:\n","        score += query_wc[word] * tf_docs[doc_id][word] # count of word in query * term_freq of the word\n","    relevance_scores[doc_id] = score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bzpnCpOYosXz","outputId":"3c35da35-d262-49b4-a844-3f4027e17168"},"source":["# lets print the relevance scores for the query\n","print(\"Document Relevancy Scores\\n\",relevance_scores)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Document Relevancy Scores\n"," {'d1': 2, 'd2': 3, 'd3': 3, 'd4': 4, 'd5': 5}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dHEuYhKGosX1"},"source":["## HOORAY !!!\n","We are done with our first simple vector space model to check the relevancy of each document to our query."]},{"cell_type":"markdown","metadata":{"id":"mvoJPBIFosX1"},"source":["### What next ?\n","This was just one the many ways you can calculate the relevancy score of query for a set of documents. I tried to comment the code as much as possible for your understanding, it is important that you get familiar with Python coding as early as possible as it will be used throughout your degree program. Please try to understand this code, as it will be useful for solving assignment 1 for this course. Regards"]}]}